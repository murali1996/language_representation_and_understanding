{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYpR5CXhVfD1"
   },
   "source": [
    "# Loading datasets and creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "LPcElNkpTp1E",
    "outputId": "42d28b13-1715-4ee5-81e6-c79405a58bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cntk in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (2.6)\n",
      "Requirement already satisfied: numpy>=1.11 in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (from cntk) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.17 in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (from cntk) (1.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 307kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: numpy>=1.8.2 in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (from scikit-learn) (1.14.3)\n",
      "Requirement not upgraded as not directly required: scipy>=0.13.3 in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (from scikit-learn) (1.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.20.1\n",
      "    Uninstalling scikit-learn-0.20.1:\n",
      "      Successfully uninstalled scikit-learn-0.20.1\n",
      "Successfully installed scikit-learn-0.20.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: zipfile36 in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (0.1.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages (4.28.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cntk\n",
    "!pip install -U scikit-learn\n",
    "!pip install zipfile36\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uF5axD1FeFOJ"
   },
   "outputs": [],
   "source": [
    "!mkdir msaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PUyrlbH2Ndkr",
    "outputId": "0d224f5f-d340-4ae7-c94a-e15a6e4c97f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ontology/Muralidhar/HyperQA/msaic\n",
      "answer.tsv  eval1_unlabelled.tsv  hyperQA_elmo_100\r\n",
      "data.tsv    glove.42B.300d.txt\t  hyperQA_elmo_100dummy.txt\r\n",
      "data.zip    glove.6B.300d.txt\t  hyperQA_elmo_20\r\n",
      "elmo2\t    glove.840B.300d.txt   hyperQA_elmo_20dummy.txt\r\n"
     ]
    }
   ],
   "source": [
    "%cd msaic\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "6BEV8YFdYDOP",
    "outputId": "378c31ec-6cd3-4299-cef3-f86e11a8280b"
   },
   "outputs": [],
   "source": [
    "!wget https://competitions.codalab.org/my/datasets/download/2c6a99a5-b071-4f1d-a3b1-d49a923e0c68 \n",
    "!mv 2c6a99a5-b071-4f1d-a3b1-d49a923e0c68 data.zip\n",
    "#!mv data.zip msaic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "oIc5WKkpaosL",
    "outputId": "8a769466-7133-4658-fe40-bc75e46447ca"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!mv glove.6B.zip msaic/\n",
    "glove_zip = r'glove.6B.zip';\n",
    "\n",
    "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "#!mv glove.840B.300d.zip msaic/\n",
    "#glove_zip = r'glove.840B.300d.zip';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rfcJb2VemQD"
   },
   "outputs": [],
   "source": [
    "#!wget https://competitions.codalab.org/my/datasets/download/a6b5bd16-db6f-4cbe-9670-04ecb7504a7a\n",
    "#!mv a6b5bd16-db6f-4cbe-9670-04ecb7504a7a Starting_Kit.zip\n",
    "#!mv Starting_Kit.zip msaic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "4NiLurmJd8Kp",
    "outputId": "a743b878-bada-45b1-8951-52e81018458a"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zipref = zipfile.ZipFile('data.zip', 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Raw Data Extracted')\n",
    "\n",
    "zipref = zipfile.ZipFile(glove_zip, 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Glove Data Extracted')\n",
    "\n",
    "'''\n",
    "zipref = zipfile.ZipFile('Starting_Kit.zip', 'r')\n",
    "zipref.extractall()\n",
    "zipref.close()\n",
    "print('Starting Kit unzipped')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "SQPWL6Hutcw9",
    "outputId": "bbce9f0f-0f55-4517-8d28-f55840967ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/ontology/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /data/ontology/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndevice_name = tf.test.gpu_device_name()\\nif device_name != '/device:GPU:0':\\n  raise SystemError('GPU device not found')\\nprint('Found GPU at: {}'.format(device_name))\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3';\n",
    "tf_sess_config = tf.ConfigProto()\n",
    "tf_sess_config.gpu_options.allow_growth = True\n",
    "'''\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "TdE2w4jkqmd4",
    "outputId": "71d83817-f29f-4811-b768-2cd3d5204237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neuvn7Yrqo86"
   },
   "outputs": [],
   "source": [
    "# Utility Funcs\n",
    "def progressBar(value, endvalue, print_vars=[''], print_values=[-1], bar_length=20):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        assert(len(print_vars)==len(print_values));\n",
    "        strg = '';\n",
    "        for var, val in zip(print_vars,print_values):\n",
    "          strg+=' {}: '.format(var.upper());\n",
    "          strg+='{:5f} |'.format(val);\n",
    "        sys.stdout.write(\"\\rPercent: [{0}][{1}/{2}] {3}% || {4} \".format(arrow + spaces, int(value), int(endvalue), int(round(percent * 100)), strg))\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "#progressBar(5,25,['loss','acc'],[5,6])\n",
    "\n",
    "def cleanText_None(str):\n",
    "  new_str = [x for x in str if x]\n",
    "  return new_str, len(new_str)\n",
    "\n",
    "def cleanText_Simple(str):\n",
    "  new_str = [x.lower() for x in re.split('\\W+', str) if x]\n",
    "  return new_str, len(new_str)\n",
    "\n",
    "def cleanText_Spaces_(str_):\n",
    "  new_str = [x for x in re.split(' ', str_) if x]\n",
    "  return new_str, len(new_str)\n",
    "def cleanText_Spaces(str_):\n",
    "  new_str = [x for x in re.split(' ', str_) if x]\n",
    "  return ' '.join(new_str), len(new_str)\n",
    "\n",
    "def cleanText_StopWords(str):\n",
    "  new_str = [x.lower() for x in re.split('\\W+', str) if (x and x not in cachedStopWords)]\n",
    "  return ' '.join(new_str), len(new_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rc_wQF_KNIBZ",
    "outputId": "8ecfefb5-2c8c-4845-b0f1-c3c3d4ebcdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the next cell, Change Line 14 and Line 22 accordingly!! \n"
     ]
    }
   ],
   "source": [
    "#cleanText = cleanText_Spaces;\n",
    "cleanText = cleanText_Simple;\n",
    "print('In the next cell, Change Line 14 and Line 22 accordingly!! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "DN8eoRjHqs5l",
    "outputId": "0298130d-52bb-46dc-9795-765dc32e1739"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5241880it [04:03, 21518.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total No.of Queries: 524193, Passages: 5241880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data and store it in a dictionary format\n",
    "data_dict = {};\n",
    "id2passage = {};\n",
    "n_queries = -1;\n",
    "n_passages = -1;\n",
    "running_query = '';\n",
    "query_len_counter = np.zeros(1000, dtype='int32');\n",
    "passage_len_counter = np.zeros(1000, dtype='int32');\n",
    "\n",
    "f = open('data.tsv','r',encoding='utf-8');\n",
    "for i, line in enumerate(tqdm(f)):\n",
    "  tokens = line.strip().split(\"\\t\")\n",
    "  query,passage,label = tokens[1],tokens[2],tokens[3];\n",
    "  _, cpassage_len = cleanText(passage);\n",
    "  passage_len_counter[np.min([999,cpassage_len])]+=1;\n",
    "  n_passages+=1;\n",
    "  id2passage[n_passages] = passage;\n",
    "  if running_query!=query:\n",
    "    running_query = query;\n",
    "    n_queries+=1;\n",
    "    data_dict[n_queries] = {};\n",
    "    _, cquery_len = cleanText(query);\n",
    "    query_len_counter[np.min([999,cquery_len])]+=1;\n",
    "    data_dict[n_queries]['query'] = query;\n",
    "    data_dict[n_queries]['query_len'] = cquery_len;\n",
    "  if int(label)==1:\n",
    "    data_dict[n_queries]['pos'] = n_passages;\n",
    "    data_dict[n_queries]['pos_len'] = cpassage_len;\n",
    "  else:\n",
    "    if 'negs' not in data_dict[n_queries]:\n",
    "      data_dict[n_queries]['negs']=[];\n",
    "      data_dict[n_queries]['negs_len']=[];\n",
    "    data_dict[n_queries]['negs'].append(n_passages);\n",
    "    data_dict[n_queries]['negs_len'].append(cpassage_len);\n",
    "print('\\nTotal No.of Queries: {}, Passages: {}'.format(n_queries+1,n_passages+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "colab_type": "code",
    "id": "z7oRVHsjqvWW",
    "outputId": "5cf27a7a-ec30-480c-fa7e-d79d80314398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length of queries: 6.9925828845482485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length of passages: 58.581230398254064\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(query_len_counter[:50]);\n",
    "plt.show()\n",
    "avg_len = np.sum([(i+1)*val for i, val in enumerate(query_len_counter)])/np.sum(query_len_counter);\n",
    "print('Avg length of queries: {}'.format(avg_len))\n",
    "plt.plot(passage_len_counter[:500]);\n",
    "plt.show()\n",
    "avg_len = np.sum([(i+1)*val for i, val in enumerate(passage_len_counter)])/np.sum(passage_len_counter);\n",
    "print('Avg length of passages: {}'.format(avg_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yyc_6aSDqxkl",
    "outputId": "21947913-b418-461a-dbd5-63e69feb5179"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 524193/524193 [00:00<00:00, 983493.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[51949, 280200, 357523, 434822, 519532] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any queries have no right answer!\n",
    "cnt1 = [];\n",
    "cnt2 = [];\n",
    "for id_ in tqdm(data_dict.keys()):\n",
    "  if 'pos' not in data_dict[id_].keys():\n",
    "    cnt1.append(id_)\n",
    "  if 'negs' not in data_dict[id_].keys():\n",
    "    cnt2.append(id_)\n",
    "print('');\n",
    "print(cnt1,cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBnQ6mm4qz1N"
   },
   "outputs": [],
   "source": [
    "# Remove queries that have no right or at least one wrong answers\n",
    "for id_ in (cnt1+cnt2):\n",
    "  del data_dict[id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "lEQ1FN6ZN6wf",
    "outputId": "5b891c7f-7716-427d-92b4-b07af20877de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "definition of heirs and assigns\n",
      "----------\n",
      "total number of americans killed in all wars\n",
      "----------\n",
      "what is the area code for san jose ca\n",
      "----------\n",
      "how much does it cost to remodel my house per square foot\n",
      "----------\n",
      "what is evolocumab injection?\n",
      "----------\n",
      "what is flit\n",
      "----------\n",
      "when must a breach be reported to us cert\n",
      "----------\n",
      "who has the record for most touchdowns\n",
      "----------\n",
      "how long is your state board license good for as a physician?\n",
      "----------\n",
      "what masters degree do you need to be a school librarian\n",
      "----------\n",
      "how long before sending out wedding thank you cards\n",
      "----------\n",
      "what is considered occupational disease\n",
      "----------\n",
      "why is important to strain urine for stone\n",
      "----------\n",
      "what does it mean when cold hurts a tooth\n",
      "----------\n",
      "what county is argyle, tx in?\n",
      "----------\n",
      "cost to furnish an apartment\n",
      "----------\n",
      "what is copyright of words\n",
      "----------\n",
      "what does carnival mean in mardi gras\n",
      "----------\n",
      "how long does concrete patio take to set\n",
      "----------\n",
      "when did ella baker born\n",
      "----------\n",
      "what is the pharynx made of\n",
      "----------\n",
      "what are decomposers in the ocean food chain\n",
      "----------\n",
      "what does pineapple slang mean\n",
      "----------\n",
      "antibiotic mouthwash\n",
      "----------\n",
      "what fertilizer do i use for iris\n",
      "----------\n",
      "paroxetine cr cost\n",
      "----------\n",
      "where is kemah tx\n",
      "----------\n",
      "how tall is tallest man in the world\n",
      "----------\n",
      "does spatial ability test depth perception\n",
      "----------\n",
      "what is truvada for\n",
      "----------\n",
      "what do florida bass eat\n",
      "----------\n",
      "who is was the photograph of iman african girl\n",
      "----------\n",
      "what is a silky cocker spaniel\n",
      "----------\n",
      "how many does the united center hold for hockey\n",
      "----------\n",
      "standard brick install rate\n",
      "----------\n",
      "what kind of soil do i use for succulents\n",
      "----------\n",
      "which type of mountain is formed due to the collision of two different kinds of plates?\n",
      "----------\n",
      "what is rhetoric?\n",
      "----------\n",
      "what is compression underwear\n",
      "----------\n",
      "how many calories are in a strawberry poptart\n",
      "----------\n",
      "what's the population of cusco peru\n",
      "----------\n",
      "where does the merengue originate\n",
      "----------\n",
      "why is element mercury used in thermometers\n",
      "----------\n",
      "what is analytic syllabus\n",
      "----------\n",
      "what is a scorm package\n",
      "----------\n",
      "what can we use instead of cream of tartar\n",
      "----------\n",
      "what channel does the flash the series air on\n",
      "----------\n",
      "what is the meaning of the blue star pins\n",
      "----------\n",
      "telephone number for grantsboro piggly wiggly\n",
      "----------\n",
      "is typhus contagious\n",
      "----------\n",
      "name an element that is chemically similar to magnesium\n",
      "----------\n",
      "what organelles are endoplasmic reticulum found in?\n",
      "----------\n",
      "who has been kicked off celebrity big brother\n",
      "----------\n",
      "sharp tv customer service number\n",
      "----------\n",
      "how to order a bottle of wine at a restaurant\n",
      "----------\n",
      "how long is army boot camp training\n",
      "----------\n",
      "who is rhea'\n",
      "----------\n",
      "how long to keep certificates of exemption\n",
      "----------\n",
      "when was the first phone made\n",
      "----------\n",
      "what does bile duct dilation mean\n",
      "----------\n",
      "does implantation occur in the side of the uterus\n",
      "----------\n",
      "who is steve harvey's ex wife\n",
      "----------\n",
      "salary range of a nutritionist\n",
      "----------\n",
      "what generation is 1979\n",
      "----------\n",
      "gene mutation vs chromosome mutation\n",
      "----------\n",
      "what creates electric current\n",
      "----------\n",
      "what state number is alaska\n",
      "----------\n",
      "why were african american men treated eqaul\n",
      "----------\n",
      "add day to file name in batch file\n",
      "----------\n",
      "what ingredients are in simethicone\n",
      "----------\n",
      "what is pemphigus disease in dogs\n",
      "----------\n",
      "what is thrombocytosis\n",
      "----------\n",
      "what is the standard length of a men's watch band\n",
      "----------\n",
      "what is aur\n",
      "----------\n",
      "when was the last volcano eruption in iceland\n",
      "----------\n",
      "what is ferrous metallurgy\n",
      "----------\n",
      "most obese states list\n",
      "----------\n",
      "how do computer worms propagate?\n",
      "----------\n",
      "meaning of lichen planus\n",
      "----------\n",
      "what actor from young and the restless just died\n",
      "----------\n",
      "can statins cause muscle cramps in legs\n",
      "----------\n",
      "is naples in south florida\n",
      "----------\n",
      "How long to roast filet mignon\n",
      "----------\n",
      "who is milton\n",
      "----------\n",
      "when should babies start rolling over on their own\n",
      "----------\n",
      "what type of electromagnetic wave is the most energetic\n",
      "----------\n",
      "the term mechanical\n",
      "----------\n",
      "what is remicade for\n",
      "----------\n",
      "average lifespan of a yellow lab\n",
      "----------\n",
      "what school district is adams twp in\n",
      "----------\n",
      "ovine definition\n",
      "----------\n",
      "what county is piscataway\n",
      "----------\n",
      "what is apache spark\n",
      "----------\n",
      "how to substitute applesauce for oil\n",
      "----------\n",
      "how much does an appraisal cost\n",
      "----------\n",
      "what does miscarriage mean\n",
      "----------\n",
      "how long does it take for nph to work\n",
      "----------\n",
      "chicory meaning\n",
      "----------\n",
      "ohkay owingeh meaning\n",
      "----------\n",
      "vitamins that fights yeast infections\n"
     ]
    }
   ],
   "source": [
    "# Printing some queries\n",
    "for q_id in np.random.choice(range(n_queries), size=100, replace=False):  \n",
    "    print('----------')\n",
    "    print(data_dict[q_id]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "byQcSYW4q121",
    "outputId": "8810c74d-c166-44be-95ea-1aaf5842b74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Not all burn is create equally. Related to point number two, not all burn rate spending is bad or good. As the creator of your business the tough decisions of what to spend money on are ultimately up to you decipher. Just remember that not all kinds of burn are created equal.\n",
      "----------\n",
      "Chicago Tourism: Best of Chicago. Chicago, Illinois. The windy city is a cornucopia of modern art, fine dining, cutting edge comedy, and die-hard sports fans. Snap a photo of your reflection in the silver Cloud Gate sculpture at Millennium Park before heading to Grant Park to get hit with the refreshing spray of Buckingham Fountain. There are dozens of museums and theater companies in Chicago, so a cultural experience is never hard to find.\n",
      "----------\n",
      "The 7 inch WiFi model was released on October 18, 2013 and the 8.9 inch WiFi model was released on November 7, 2013 in the United States. In September 2014, Amazon released the second generation of the Fire HDX 8.9 model that has a faster processor and a more powerful graphics processing unit.\n",
      "----------\n",
      "The memorial won’t honour soldiers, sailors, fliers or even the Coast Guard however. Instead it will pay tribute to the tens of thousands of dogs that have served the United States military since World War Two.\n",
      "----------\n",
      "In women, the adrenal glands and ovaries produce small amounts of testosterone. Before a boy is even born, testosterone is working to form male genitals. During puberty, testosterone is responsible for the development of male attributes like a deeper voice, beard, and body hair.he Effects of Testosterone on the Body. Testosterone is an important male hormone. A male begins to produce testosterone as early as seven weeks after conception. Testosterone levels rise during puberty, peak during the late teen years, and then level off.\n",
      "----------\n",
      "The jigger is named for the unit of liquid it typically measures, a jigger or shot, which is historically defined as 2 handfuls (2 fluid ounces) or 1⁄2 of a jack in the traditional binary submultiple (a.k.a. English doubling) system.\n",
      "----------\n",
      "Best Answer: ok, found this answer. hope it helps Wine gums originated in the UK. Charles Riley Maynard started his business in 1880 by producing confections in a candy kitchen with his brother Tom, while his wife Sarah Ann, served the customers.Maynard's candy grew steadily and was launched as a company in 1896.aynard the younger wished to market his candies as being so good that they should be appreciated like a fine wine. Therefore he called them wine gums and labeled them with wine names. Source(s): http://www.hungrybrowser.com/phaedrus/m0...\n",
      "----------\n",
      "Home SparkNotes → Short Story Study Guides → The Jilting of Granny Weatherall → Plot Overview. Sick in bed, Granny Weatherall is being visited by Doctor Harry, a man whom she considers little more than a child. Saying there is nothing wrong with her, Granny orders the doctor to leave. He speaks in a condescending tone to her, even after she snaps at him.\n",
      "----------\n",
      "1 Vaginal yeast infections are caused by the same fungus that causes oral thrush. 2  Although a yeast infection isn't dangerous, if you're pregnant you can pass the fungus to your baby during delivery. 3  As a result, your newborn may develop oral thrush.\n",
      "----------\n",
      "Riverside County and San Bernardino County to the north form this metro area. Like many such metro areas in the Southwest, it extends far into uninhabited desert areas, in this case east through the Mojave Desert to the Nevada/Arizona border. Larger than nine U.S. states, it is often referred to as the Inland Empire.\n",
      "----------\n",
      "Although you should have your hair trimmed regularly, some people may need it more often than others. If you have short hair, it's recommended that you get a cut every 4-6 weeks.If you're growing your hair long, don't forget you still also need to have regular trims – growing long hair shouldn’t mean you should skip the hairdresser altogether! Trims every 6-8 weeks ensure hair grows out evenly and healthy.If you color your hair or use any other type of chemical treatment on it, this usually results in drier hair. Dry hair tends to break easier and develop more split ends, meaning you’ll need more frequent trims (every 4-6 weeks) even if your hair is long.rims every 6-8 weeks ensure hair grows out evenly and healthy. If you color your hair or use any other type of chemical treatment on it, this usually results in drier hair. Dry hair tends to break easier and develop more split ends, meaning you’ll need more frequent trims (every 4-6 weeks) even if your hair is long.\n",
      "----------\n",
      "Cold Brew Usually Has More Caffeine. In general, cold-brewed coffee is more caffeinated than hot-brewed coffee. The difference is especially noticeable when you use a full-immersion brew method to make toddy, which is why toddy is typically cut with either water or milk. Even a cold brew made with a drip method, though, can have more caffeine than a hot-brewed coffee.\n",
      "----------\n",
      "Aporia is an expression of insincere doubt. Clear definition and great examples of Aporia. This article will show you the importance of Aporia and how to use it.\n",
      "----------\n",
      "The majority of the new residents, as before, came from the Old South, but the 1870s also saw a surge of Germans into Leon County, and by 1880 the census reported slightly more than 900 German-born residents, about 7 percent of the population.\n",
      "----------\n",
      "Sarsaparilla binds with toxins and detoxifies the blood. Sarsaparilla contains saponin compounds,[1] that are not easily digested, that bind with endotoxin bacteria byproducts and remove them from the body via urine or stool. Sarsaparilla is also a diaphoretic and promotes the excretion of toxins through sweating.[2]\n",
      "----------\n",
      "Gondola at Sulphur Mountain, Alberta. A gondola lift, as opposed to a cable car, is a means of cable transport and type of aerial lift which is supported and propelled by cables from above. It consists of a loop of steel cable that is strung between two stations, sometimes over intermediate supporting towers.\n",
      "----------\n",
      "The average salary for a senior pastor at a church with a $10 million or more budget is $189,000. The median age for the senior pastor in surveyed churches is 49. Most of the surveyed churches have some debt, but the amount is equal to or less than the total amount of its annual budget.\n",
      "----------\n",
      "Empowered with the sovereign authority of the people by the framers and the consent of the legislatures of the states, it is the source of all government powers, and also provides important limitations on the government that protect the fundamental rights of United States citizens.\n",
      "----------\n",
      "Recruiters work on behalf of companies to identify qualified people who have both the required skills and the right attitude for the job. What many people don't realize is that recruiters work on behalf of our clients; they tell me the job they need filled and I search for talented people to do the work. Some of these openings are easy to fill, others require months of work to find just the right person.\n",
      "----------\n",
      "It is determined as the value of the stress at which a line of the same slope as the initial portion (elastic region) of the curve that is offset by a strain of 0.2% or a value of 0.002 strain intersects the curve. In our example, the 0.2% offset yield strength is a 88 ksi. This is a very important aspect of strength.\n"
     ]
    }
   ],
   "source": [
    "# Printing some passages\n",
    "for i in np.random.choice(range(n_passages), size=20, replace=False):\n",
    "    print('----------')\n",
    "    print(id2passage[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7177
    },
    "colab_type": "code",
    "id": "m-UFz7hTq37p",
    "outputId": "2a5326c9-8455-4eb3-b98e-421b7ee7953a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "redwood decking cost per linear foot\n",
      "Routinely staining your deck will help protect your deck but it is not uncommon for pressure treated decking to split, crack, warp, and turn gray in a few years. Cedar and redwood decking prices range from about $1.25 to $2.00 per linear foot for 5/4x6 stock.Redwood is commonly available on the west coast but is scarce in the eastern United States.igher grade materials and long lengths will cost a premium. With over 100 brands to choose from there is a wide range of prices for vinyl and composite decking. The higher end name brands usually cost between $3.00 to $4.00 per linear foot. The big box stores offer economy composite materials for about half the price.\n",
      "---------\n",
      "The average cost of basic cedar decking is $4.63 per linear foot. The minimum cost of basic cedar decking is $3.89 per linear foot. The maximum cost of basic cedar decking is $5.39 per linear foot.his includes treated cedar decking. The minimum cost of mid-grade quality cedar decking is $4.51 per linear foot. The maximum cost of mid-grade cedar decking is $6.59 per linear foot.\n",
      "---------\n",
      "High Quality. The average cost of the highest quality of cedar decking is $6.56 per linear foot. The minimum cost of the highest quality of cedar decking is $5.43 per linear foot. The maximum cost of the highest quality of cedar decking is $7.70 per linear foot.his includes treated cedar decking. The minimum cost of mid-grade quality cedar decking is $4.51 per linear foot. The maximum cost of mid-grade cedar decking is $6.59 per linear foot.\n",
      "---------\n",
      "Mid-Grade. The average cost of mid-grade cedar decking is $5.62 per linear foot. This includes treated cedar decking. The minimum cost of mid-grade quality cedar decking is $4.51 per linear foot.The maximum cost of mid-grade cedar decking is $6.59 per linear foot.his includes treated cedar decking. The minimum cost of mid-grade quality cedar decking is $4.51 per linear foot. The maximum cost of mid-grade cedar decking is $6.59 per linear foot.\n",
      "---------\n",
      "1 In addition, unless you want the wood to weather to a pale gray, you’ll need to oil the deck every one to two years. 2  An ipe deck can cost $23-30 per square foot. 3  Composite-The old style of composite decking is a blend of wood fiber and plastic which has a brushed, matte surface. Composite railing ranges from $15-$30 per linear foot before installation costs. 2  Cable rail-Cable rail is a wonderful solution for properties with a view. 3  The slender metal cables run horizontally, and take up so little visual space that you can see the trees, mountains, or ocean beyond.\n",
      "---------\n",
      "1 A capped composite deck can cost $20-28 per square foot. 2  Cedar or redwood-Cedar or redwood are good mid-range options in that they cost less than tropical hardwoods or composite, yet last 15 to 20 years. Composite railing ranges from $15-$30 per linear foot before installation costs. 2  Cable rail-Cable rail is a wonderful solution for properties with a view. 3  The slender metal cables run horizontally, and take up so little visual space that you can see the trees, mountains, or ocean beyond.\n",
      "---------\n",
      "In most regions of the country, redwood and cedar each cost at least three times more than pressure-treated lumber. For example, I recently bought an 8-ft.-long red cedar 2 x 6 and paid nearly $4 per linear foot--ouch!Both species are considerably less expensive on the West Coast. In California, for instance, a B-grade redwood 2 x 6 costs about $2.35 per linear foot.Redwood and cedar require an annual power washing and coat of finish every three to four years.rices vary because there are so many different companies, but composites are typically less expensive than plastic lumber. For example, for 5/4 x 6-­in. decking, expect to pay about $2.50 per linear foot for composites, and $3 per linear foot for plastic lumber.\n",
      "---------\n",
      "Prices vary because there are so many different companies, but composites are typically less expensive than plastic lumber. For example, for 5/4 x 6-­in. decking, expect to pay about $2.50 per linear foot for composites, and $3 per linear foot for plastic lumber.Most composite decking and plastic lumber manufacturers also offer a line of handrails, balusters, fascias and other decorative trim.rices vary because there are so many different companies, but composites are typically less expensive than plastic lumber. For example, for 5/4 x 6-­in. decking, expect to pay about $2.50 per linear foot for composites, and $3 per linear foot for plastic lumber.\n",
      "---------\n",
      "1 An ipe deck can cost $23-30 per square foot. 2  Composite-The old style of composite decking is a blend of wood fiber and plastic which has a brushed, matte surface. 3  While it requires periodic cleaning to look good, it costs less than newer types of capped composite which need little care. Composite railing ranges from $15-$30 per linear foot before installation costs. 2  Cable rail-Cable rail is a wonderful solution for properties with a view. 3  The slender metal cables run horizontally, and take up so little visual space that you can see the trees, mountains, or ocean beyond.\n",
      "---------\n",
      "Higher grade materials and long lengths will cost a premium. With over 100 brands to choose from there is a wide range of prices for vinyl and composite decking. The higher end name brands usually cost between $3.00 to $4.00 per linear foot.The big box stores offer economy composite materials for about half the price.igher grade materials and long lengths will cost a premium. With over 100 brands to choose from there is a wide range of prices for vinyl and composite decking. The higher end name brands usually cost between $3.00 to $4.00 per linear foot. The big box stores offer economy composite materials for about half the price.\n",
      "**************\n",
      "what are other words for wind\n",
      "Synonyms-adjectives. 1  blowing. 2  gusty. 3  blustering. 4  windy. 5  squally. 6  boisterous (violent). 7  flatulent. 8  stormy. 9  pulmonic. 10  breezy. 11  tempestuous. 12  pulmonary.\n",
      "---------\n",
      "1 A sudden gust of wind circled them and whispered words in her mind. 2  Carmen was so groggy that the icy wind failed to keep her awake. 3  A cold wind tore at her hair as she stomped across the courtyard and out to the chicken coup.\n",
      "---------\n",
      "Wind turbines, like aircraft propeller blades, turn in the moving air and power an electric generator that supplies an electric current. Simply stated, a wind turbine is the opposite of a fan. Instead of using electricity to make wind, like a fan, wind turbines use wind to make electricity.\n",
      "---------\n",
      "Free thesaurus definition of types of wind from the Macmillan English Dictionary-a free English dictionary online with thesaurus and with pronunciation from Macmillan Publishers Limited.\n",
      "---------\n",
      "1 Carmen was so groggy that the icy wind failed to keep her awake. 2  A cold wind tore at her hair as she stomped across the courtyard and out to the chicken coup. 3  The snow started shortly after they left and the wind blew it horizontal. 4  At any time a gust of wind could send the car over the cliff.\n",
      "---------\n",
      "Synonyms-noun. 1  wind. 2  efflation. 3  breath of air. 4  blow. 5  current. 6  blast. 7  half a gale. 8  whirlwind. 9  anticyclone. 10  harmattan. 11  mistral. 12  capful of wind. 13  blizzard. 14  foehn. 15  wuther. 16  dirty weather. 17  mare's tail. 18  anemography. 19  weathervane. 20  insufflation. 21  blowing. 22  errhine. 23  hiccup. 24  Boreas. 25  air blower. 26  fan. 27  gills. 28  draught. 29  eluvium. 30  puff. 31  drift. 32  jet stream. 33  breeze. 34  storm.\n",
      "---------\n",
      "Wind is a form of solar energy. Winds are caused by the uneven heating of the atmosphere by the sun, the irregularities of the earth's surface, and rotation of the earth. Wind flow patterns are modified by the earth's terrain, bodies of water, and vegetative cover.\n",
      "---------\n",
      "1 A cold wind tore at her hair as she stomped across the courtyard and out to the chicken coup. 2  The snow started shortly after they left and the wind blew it horizontal. 3  At any time a gust of wind could send the car over the cliff.\n",
      "---------\n",
      "Other words that mean: aqua, water, wind, or storm? I'm looking for uncommon words (from the English language) that mean things like aqua, water, wind, or storm. It can also mean some sort of phrase.\n",
      "---------\n",
      "Wind Turbines. Wind turbines, like aircraft propeller blades, turn in the moving air and power an electric generator that supplies an electric current. Simply stated, a wind turbine is the opposite of a fan. Instead of using electricity to make wind, like a fan, wind turbines use wind to make electricity.\n",
      "**************\n",
      "when was peter rabbit patented\n",
      "Today, The Tale of Peter Rabbit remains one of the best-selling children's classics of all time and The World of Beatrix Potter™, initiated by Beatrix herself in 1903, is one of the world's largest international literature-based licensing programmes.\n",
      "---------\n",
      "Peter Rabbit Loses Copyright Protection. Friday, January 31, 2014 by KLM, Inc. In the U.S., and many other countries, copyright protection lasts until the death of the creator plus 70 years. Recently dubbed “Public Domain Day,” January 1st each year marks the end of copyright protection for works created by individuals who died 70 years earlier.\n",
      "---------\n",
      "In 2014, creators who died in 1943 fall into the public domain. One of those authors is Beatrix Potter, the author and illustrator who wrote The Tale of Peter Rabbit, making her works available for free use.\n",
      "---------\n",
      "In some other countries, including Canada, copyright term is life plus 50 years, and in those countries C.S. Lewis, Aldous Huxley, Robert Frost, Jean Cocteau, and the famous poet Sylvia Path, who all died in 1963, enter the public domain in 2014.\n",
      "---------\n",
      "The Tale of Peter Rabbit is a British children's book written and illustrated by Beatrix Potter that follows mischievous and disobedient young Peter Rabbit as he is chased about the garden of Mr. McGregor. He escapes and returns home to his mother, who puts him to bed after dosing him with camomile tea.\n",
      "---------\n",
      "Warne's New York office failed to register the copyright for The Tale of Peter Rabbit in the United States, and unlicensed copies of the book (from which Potter would receive no royalties) began to appear in the spring of 1903.\n",
      "---------\n",
      "In 1902, Beatrix had omitted to secure the copyright of The Tale of Peter Rabbit in the United States. The ensuing loss of royalties had taught her a valuable lesson: she photographed her finished Peter Rabbit doll and registered it immediately at the Patent Office.\n",
      "---------\n",
      "Beatrix Potter, design for the back cover of 'Peter Rabbit's Painting Book', 1911. Museum no. BP.1103IV, © Frederick Warne & Co., Ltd.\n",
      "---------\n",
      "In 2002, Frederick Warne (by now an imprint of Penguin Books) issued a new edition of the Original Peter Rabbit Books with plain, pale-blue endpapers in keeping with what Potter herself originally specified. Unlike many of her contemporaries, Potter was both a distinguished illustrator and a storyteller.\n",
      "---------\n",
      "The story was inspired by a pet rabbit Potter had as a child, which she named Peter Piper. Through the 1890s, Potter sent illustrated story letters to the children of her former governess, Annie Moore.\n",
      "**************\n",
      "what year was kennedy shot?\n",
      "53 Years Since JFK Assassination. November 22, 2016. The 35th president of the United States was shot by Lee Harvey Oswald on Nov. 22, 1963. U.S. President John F. Kennedy was assassinated on Nov. 22, 1963, when he was traveling through Dallas, Texas, in an open-top convertible. The 35th president of the United States was shot by Lee Harvey Oswald as the motorcade passed the Texas School Book Depository.\n",
      "---------\n",
      "LBJ Gets the Blame. Robert McHeffey and Paul M. are grabbing a smoke on Commerce Street in downtown Dallas, just blocks from Dealey Plaza where President John F. Kennedy was shot 50 years ago today. A light drizzle falls on his uncharacteristically cool, 39-degree day.\n",
      "---------\n",
      "JFK Assassination: Dispatches From Dallas on 50th Anniversary. President John F. Kennedy was assassinated Nov. 22, 1963, while riding in an open car in a motorcade at Dealey Plaza in Dallas. Despite the Warren Commission Report's conclusion that the gunman acted alone, the event is shrouded in conspiracy theories.\n",
      "---------\n",
      "53 Years Since JFK Assassination. The 35th president of the United States was shot by Lee Harvey Oswald on Nov. 22, 1963. U.S. President John F. Kennedy was assassinated on Nov. 22, 1963, when he was traveling through Dallas, Texas, in an open-top convertible.\n",
      "---------\n",
      "Search form. John F. Kennedy. John F. Kennedy was the 35th President of the United States (1961-1963), the youngest man elected to the office. On November 22, 1963, when he was hardly past his first thousand days in office, JFK was assassinated in Dallas, Texas, becoming also the youngest President to die.\n",
      "---------\n",
      "This Day In History: 11/22/1963 - Kennedy Assassinated. See what happened in history on November 22 by watching the video of This Day in History. On November 22, 1906, Morse Code was accepted as the universal S.O.S. signal for ships in distress. On November 22, 1986, Mike Tyson made history.\n",
      "---------\n",
      "History & Culture. Shortly after midnight on June 5, 1968, presidential candidate Robert F. Kennedy was shot three times by Palestinian immigrant Sirhan Sirhan after giving a speech at the Ambassador Hotel in Los Angeles, California. Robert Kennedy died of his wounds 26 hours later.\n",
      "---------\n",
      "The School Book Depository where Oswald shot Kennedy is now the Sixth Floor Museum, featuring exhibits surrounding the tragedy. The conspiracy theories are still around because people don't know what to believe, said museum curator Gary Mack, who admits he's not satisfied with the official story..\n",
      "---------\n",
      "Most importantly, on November 22, 1963, President John F. Kennedy was assassinated in Dallas, Texas. He was shot twice, and an hour after his death Lee Harvey Oswald was arrested for the crime. This event left the nation in mourning.\n",
      "---------\n",
      "Since 1976, Dave Perry has been researching John F. Kennedy's killing. Top conspiracy theories include the CIA, the mob and then-Vice President Lyndon Johnson. Event in Pittsburgh drew Oliver Stone and hundreds of conspiracy theorists. Perry can't debunk a CIA conspiracy theory; many CIA JFK documents still classified. 1  During the half century since President John F. Kennedy was assassinated, you may have heard about a few conspiracy theories.\n",
      "**************\n",
      "do I include w2 when mailing my federal return\n",
      "Relevance. Rating Newest Oldest. Best Answer: you will send the federal copy of your w2 with your federal return and the state copy with your state return which should leave you with 2 copies for your records. You can see which copies are which by looking at the bottom of the w2 and it should say.Now the state tax return usually shows an address of where to mail the return, but the federal never does. Depending on where you live you will have a different address from any other state.ou can see which copies are which by looking at the bottom of the w2 and it should say. Now the state tax return usually shows an address of where to mail the return, but the federal never does. Depending on where you live you will have a different address from any other state.\n",
      "---------\n",
      "Form W-2 is Wage and Tax Statement. If you're filing a paper state return, then you attach Copy 2-To Be Filed with Employee's State, City, or Local Income Tax Return. If you're filing electronically, then you keep Copy 2 with your other tax records for the year.or federal returns, the only 1099 form to send is Form 1099-R and only if it shows federal withholding. Do not send any other type of 1099 and do not send any 1099-R that doe … s not show withholding.\n",
      "---------\n",
      "There is no action you need to take at this time. If additional information is needed, you will be contacted by mail. In the future, when preparing your return, enter your W-2, 1099 and/or MI-4919 information on the Schedule W. A Schedule W is required with each return.here is no action you need to take at this time. If additional information is needed, you will be contacted by mail. In the future, when preparing your return, enter your W-2, 1099 and/or MI-4919 information on the Schedule W. A Schedule W is required with each return.\n",
      "---------\n",
      "For federal returns, the only 1099 form to send is Form 1099-R and only if it shows federal withholding. Do not send any other type of 1099 and do not send any 1099-R that doe … s not show withholding.Check the instructions for your state return.or federal returns, the only 1099 form to send is Form 1099-R and only if it shows federal withholding. Do not send any other type of 1099 and do not send any 1099-R that doe … s not show withholding.\n",
      "---------\n",
      "If you are filing Form 5402EZ, do not attach a copy of your federal return to your California return.f you are filing Form 5402EZ, do not attach a copy of your federal return to your California return.\n",
      "---------\n",
      "1 Place a copy of your federal return and schedules behind your Minnesota forms. 2  Do not send in your W-2 or 1099 forms. 3  Do not use staples on your return. 4  If you want to ensure that your papers stay together, use a paper clip.e sure to include enough postage to avoid having your forms returned to you by the U.S. Postal Service. You’ll probably need additional postage if you enclose more than three sheets of paper. For information on when you must mail your return, see Due Dates for Filing Individual Income Tax Return.\n",
      "---------\n",
      "Do not send any of the following with your Utah return: 1  Your federal return, unless you are amending your Utah return based on a change to the federal return. 2  Your original Utah return when amending the Utah return. 3  Certification paperwork, worksheets, or credit schedules, unless listed in the table above.nclude the following documents with your TC-40 when filing a paper return. See Income Tax Return Mailing Addresses for the correct mailing address to use for your return. Do not staple documents together.\n",
      "---------\n",
      "Include the following documents with your TC-40 when filing a paper return. See Income Tax Return Mailing Addresses for the correct mailing address to use for your return. Do not staple documents together.Most Utah returns will include: 1  The original signed Utah tax return (TC-40). 2  Only include page 3 of the TC-40 if you have entered information on that page.nclude the following documents with your TC-40 when filing a paper return. See Income Tax Return Mailing Addresses for the correct mailing address to use for your return. Do not staple documents together.\n",
      "---------\n",
      "no, you send in your federal copy to your federal taxes and you save your state copy for your state taxes-go to your state tax site-do a google search-federal and state taxes are two separate things.ou can see which copies are which by looking at the bottom of the w2 and it should say. Now the state tax return usually shows an address of where to mail the return, but the federal never does. Depending on where you live you will have a different address from any other state.\n",
      "---------\n",
      "Best Answer: you will send the federal copy of your w2 with your federal return and the state copy with your state return which should leave you with 2 copies for your records.ou can see which copies are which by looking at the bottom of the w2 and it should say. Now the state tax return usually shows an address of where to mail the return, but the federal never does. Depending on where you live you will have a different address from any other state.\n",
      "**************\n",
      "how wide should supports be for staircase\n",
      "The total run is the overall horizontal distance traveled by the stringer. The total rise is a stairway’s overall change in height, from the landing pad to the top of the deck’s decking. Now, some common code requirements, along with our recommendations: - The stair’s treads should be at least 36 inches wide. We think stairs should be at least 48 inches wide, so they don’t feel cramped. - The maximum allowable unit rise is 7 3/4 inches, and the minimum unit rise is 4 inches. For recommendations on rise-run combos, see the tip.\n",
      "---------\n",
      "The most critical characteristic of stairs, even more important than the size of any of the parts, is that every step be the same. In fact, building codes enforce this rule, with some exceptions for the first and last step.\n",
      "---------\n",
      "Measure the height of the area where you will install the stairs. This is also called the total rise. If you don’t plan to make the top step level with the area where the stairs begin (ex. deck, doorstop, etc.), be sure to account for this gap in your measurement.\n",
      "---------\n",
      "A stairway, staircase, stairwell, flight of stairs, or simply stairs is a construction designed to bridge a large vertical distance by dividing it into smaller vertical distances, called steps. Stairs may be straight, round, or may consist of two or more straight pieces connected at angles.\n",
      "---------\n",
      "Establish the run of each step to get the total run of the staircase. The run of each step should be no less than 9 inches (23 cm) and realistically at least 10 inches (25 cm). Multiply the total number of steps by the run of each step: 13 total steps x 10 inches = 130 inches (330 cm) total run. The total run is the horizontal distance the stairs will travel from beginning to end.\n",
      "---------\n",
      "Best Answer: the best thing u can do is contact city hall.they will be able to tell u what the minimum and the maximum depth of ur treads should be. where i live it is no less than 9 1/4 inches in depth,and no taller than 8 1/4 inches from top of tread to top of tread. it is hard to tell u on here how to make the cuts in order to make ur stringers. e-mail me and i will help u out as much as possible. Source(s): home builder for 12 years.\n",
      "---------\n",
      "Most fire codes do not allow stairs to rise more than 12 feet without providing a landing. The length of the landing should be at least equal to the width of the stair tread. According to the 1996 CABO code, the openings between balusters are to be no greater than 4 inches.\n",
      "---------\n",
      "The ratio of unit rise to unit run determines the angle of the stairs. The 1996 Council of American Building Officials (CABO) and the 2000 International Code Council recommendations call for unit runs to be not less than 10 inches and unit rises not more than 7¾ inches.\n",
      "---------\n",
      "The measurements of a stair, in particular the rise height and going of the steps, should remain the same along the stairs. The following stair measurements are important: The rise height or rise of each step is measured from the top of one tread to the next.\n",
      "---------\n",
      "Maximum Rise 220 mm (8.7 in) and Minimum Going 220 mm (8.7 in) remembering that the maximum pitch of private stairs is 42°. The normal relationship between dimensions of the rise and going is that twice the rise plus the going (2R + G) should be between 550 and 700 mm (21.7 and 27.6 in)\n",
      "**************\n",
      "do converse sneakers come in wide\n",
      "The shoe that never seems to go out of style! This season ramp up your wardrobe with these CT OX sneakers from the Converse collection. There are a wide range of colors to choose from to...Converse CT OX Sneaker Shiekh Shoes Converse $50.00.he shoe that never seems to go out of style! This season ramp up your wardrobe with these CT OX sneakers from the Converse collection. There are a wide range of colors to choose from to...\n",
      "---------\n",
      "shoes for men converse wide. Follow shoes for men converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow shoes for men converse wide to stop getting updates on your eBay Feed.hoes for men converse wide. Follow shoes for men converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow shoes for men converse wide to stop getting updates on your eBay Feed.\n",
      "---------\n",
      "$44.99. The Converse brand of athletic shoes, which now includes a wide range of lace-up and slip-on styles for men and women in a dazzling array of hip materials, colors, and patterns, first... CONVERSE CHUCK TAYLOR FX OX 102081F...Amazon.com Converse $44.99 Free Shipping.See More Converse Shoes.he Converse brand of athletic shoes, which now includes a wide range of lace-up and slip-on styles for men and women in a dazzling array of hip materials, colors, and patterns, first... CONVERSE CHUCK TAYLOR FX OX 102081F... Amazon.com Converse $44.99 Free Shipping. See More Converse Shoes.\n",
      "---------\n",
      "So as I'm skimming the internet for shoes (because that's what I do in my spare time), I come across an Amazon commenter who *swore* that Converse makes wide width Chuck Taylor All-Stars on request.Yeah, it was a random comment, but I had to follow through on it.here's a few issues, too. The shoes are probably a 2E width; I still had to go up a size and a half to 13 to squeeze my 4E feet into them. Even with that, my feet felt much better after I had switched back to CLE's.\n",
      "---------\n",
      "This feature is not available right now. Please try again later. I am doing this video about Converse Chuck Taylor All Star shoes for wide feet for people with wide feet also to include to put on the JAFTA thing to add Converse to be purchase by a Japanese company to overtake Nike as basketball shoe dominance.Please check out my videos about JAFTA on Embargoman:his feature is not available right now. Please try again later. I am doing this video about Converse Chuck Taylor All Star shoes for wide feet for people with wide feet also to include to put on the JAFTA thing to add Converse to be purchase by a Japanese company to overtake Nike as basketball shoe dominance.\n",
      "---------\n",
      "Follow converse wide to get e-mail alerts and updates on your eBay Feed.Unfollow converse wide to stop getting updates on your eBay Feed.Yay! You're now following converse wide in your eBay Feed.ollow converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow converse wide to stop getting updates on your eBay Feed. Yay! You're now following converse wide in your eBay Feed.\n",
      "---------\n",
      "The Converse brand of athletic shoes, which now includes a wide range of lace-up and slip-on styles for men and women in a dazzling array of hip materials, colors, and patterns, first...CONVERSE CHUCK TAYLOR FX OX 102081F...Amazon.com Converse $44.99 Free Shipping.See More Converse Shoes.he Converse brand of athletic shoes, which now includes a wide range of lace-up and slip-on styles for men and women in a dazzling array of hip materials, colors, and patterns, first... CONVERSE CHUCK TAYLOR FX OX 102081F... Amazon.com Converse $44.99 Free Shipping. See More Converse Shoes.\n",
      "---------\n",
      "converse wide. Follow converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow converse wide to stop getting updates on your eBay Feed. Yay! You're now following converse wide in your eBay Feed.ollow converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow converse wide to stop getting updates on your eBay Feed. Yay! You're now following converse wide in your eBay Feed.\n",
      "---------\n",
      "Best Answer: I would suggest just getting your regular size. I have wide feet and usually don't wear flats are skinny shoes like Converse because they look odd on wide feet, but Converse make your feet look like a regular size.I wear a size 7W, but I got black Converse in a 7R.Hope it helps!est Answer: I would suggest just getting your regular size. I have wide feet and usually don't wear flats are skinny shoes like Converse because they look odd on wide feet, but Converse make your feet look like a regular size. I wear a size 7W, but I got black Converse in a 7R. Hope it helps!\n",
      "---------\n",
      "Follow shoes for men converse wide to get e-mail alerts and updates on your eBay Feed.hoes for men converse wide. Follow shoes for men converse wide to get e-mail alerts and updates on your eBay Feed. Unfollow shoes for men converse wide to stop getting updates on your eBay Feed.\n",
      "**************\n",
      "which accelerating plate is positive?\n",
      "An electron is accelerated inside a parallel plate capacitor. The electron leaves the negative plate with a negligible initial velocity and then after the acceleration it hits the positive plate with a final velocity ?. The distance between the plates is 18.0 cm, and the voltage difference is 129 kV.\n",
      "---------\n",
      "SOLVE IT. (A) Find the speed of the particle at by modeling it as a particle under constant acceleration. Conceptualize When the positive charge is placed at, it experiences an electric force toward the right in the figure due to the electric field directed toward the right. Categorize Because the electric field is uniform, a constant electric force acts on the charge.\n",
      "---------\n",
      "If the spacing between the plates is doubled, how is the magnitude of charge on the plates affected? 64. •• A charge of 24.5 μC is located at (4.40 m, 6.02 m), and a charge of −112. μC is located at (.−450m,6.75m).\n",
      "---------\n",
      "(c) Calculate the magnitude of the charge on the plates if the separation is 0.90 mm. .m0066 2 49. •• Suppose that after walking across a carpeted floor you reach for a doorknob and just before you touch it a spark jumps 0.50 cm from your finger to the knob.\n",
      "---------\n",
      "Example 23.9 An Accelerating Positive Charge: Two Models. A uniform electric field is directed along the x axis between parallel plates of charge separated by a distance d as shown in the figure.\n",
      "---------\n",
      "What capacitance is needed to store this much charge in a capacitor with a potential difference between its plates of 9.0 V? 43. •• A parallel-plate capacitor is made from two aluminum-foil sheets, each 4.3 cm wide and 5.1 m long. Between the sheets is a Teflon strip of the same width and length that is 0.025 mm thick.\n",
      "---------\n",
      "× 5 NC points in the positive x direction. Find the change in electric potential energy of a 45.- Cμ charge as it moves from the origin to the points (a) (0, 6.0 m); (b) (6.0 m, 0); and (c) (6.0 m, 6.0 m).\n",
      "---------\n",
      "home / study / questions and answers / science / advanced physics / an electron is accelerated inside a parallel plate ...\n",
      "---------\n",
      "A positive point charge q of mass m is released from rest at a point next to the positive plate and accelerates to a point next to the negative plate. (A) Find the speed of the particle at by modeling it as a particle under constant acceleration. Conceptualize When the positive charge is placed at, it experiences an electric force toward the right in the figure due to the electric field directed toward the right.\n",
      "---------\n",
      "(a) The electric field between the plates; (b) the potential difference between the plates; (c) the capacitance; (d) the energy stored in the capacitor. 18. A parallel-plate capacitor is connected to a battery that maintains a constant potential difference V between the plates.\n",
      "**************\n",
      "where is ayutthaya located\n",
      "Wat Phra Si Sanphet. Founded around 1350, Ayutthaya became the second capital of Siam after Sukhothai. Throughout the centuries, the ideal location between China, India and the Malay Archipelago made Ayutthaya the trading capital of Asia and even the world. By 1700 Ayutthaya had become the largest city in the world with a total of 1 million inhabitants.\n",
      "---------\n",
      "Asia » Thailand » Central » Ayuthaya » Sri Ayutthaya. Just 80 kilometers (50 miles) north of Bangkok is the old capitol of Ayutthaya (or Ayuthaya, or even Ayodhaya. No matter how you spell it, its pronounced ah-you-tah-ya.) The city became Thailand's capitol in the mid-14th century and remained the capitol until the late 18th century.\n",
      "---------\n",
      "Ayutthaya City. Just 80 kilometers (50 miles) north of Bangkok is the old capitol of Ayutthaya (or Ayuthaya, or even Ayodhaya. No matter how you spell it, its pronounced ah-you-tah-ya.) The city became Thailand's capitol in the mid-14th century and remained the capitol until the late 18th century.\n",
      "---------\n",
      "Wat Phra Si Sanphet. Founded around 1350, Ayutthaya became the second capital of Siam after Sukhothai. Throughout the centuries, an ideal location between China, India, and the Malay Archipelago made Ayutthaya the trading capital of Asia and even the world.\n",
      "---------\n",
      "There were three palaces in Ayutthaya: Grand Palace, Chantharakasem Palace (the Front Palace), and Wang Lang (the Rear Palace). In addition, there were many other palaces and buildings for royal visits located outside Ayutthaya, such as the palace at Bang Pa-In and Nakhon Luang Building at Nakhon Luang.\n",
      "---------\n",
      "Ayutthaya Historical Park (Thai: อุทยานประวัติศาสตร์พระนครศรีอยุธยา (Pronunciation)) covers the ruins of the old city of Ayutthaya, Thailand. The city of Ayutthaya was founded by King Ramathibodi I in 1351. The city was captured by the Burmese in 1569. Though not pillaged, it lost many valuable and artistic objects..\n",
      "---------\n",
      "Ayutthaya ruins at dusk. The Ayutthaya Historical Park comprises of the ruins of temples and palaces of the capital of the ancient Ayutthaya Kingdom. The park is located on an island surrounded by three rivers where the old capital used to be.\n",
      "---------\n",
      "The stunning old capital of Thailand was amazing to see. Even if you are not a history enthusiast, the sheer magnitude of these ruins is a must see. The detailed stone work and the Bhuddahs that are 700 years old should be seen. Take lots of water, good walking shoes and ladies should have a long dress or slacks and...\n",
      "---------\n",
      "How to get to Ayutthaya Historical Park. The park is located on an island surrounded by the Chao Phraya, Pa Sak and Lopburi rivers in the town of Ayutthaya, 80 kilometers North of Bangkok. You can get there by private taxi, river boat, train or bus.\n",
      "---------\n",
      "In the center of island-city of Ayutthaya, TripAdvisor designated attractions Ayutthaya Ruins & Wat Mahathat are comparable to Mayan ruins in Copan, Honduras. Moreover, outside the main complex there is a collection of ruins that are not yet undergoing preservation and reconstruction.\n",
      "**************\n",
      "what are viruses quizlet\n",
      "Virus: A microorganism that is smaller than a bacterium that cannot grow or reproduce apart from a living cell. A virus invades living cells and uses their chemical machinery to keep itself alive and to replicate itself. It may reproduce with fidelity or with errors (mutations); this ability to mutate is responsible for the ability of some viruses to change slightly in each infected person, making treatment difficult.\n",
      "---------\n",
      "Classes of Malicious Software. Two of the most common types of malware are viruses and worms. These types of programs are able to self-replicate and can spread copies of themselves, which might even be modified copies. To be classified as a virus or worm, malware must have the ability to propagate.\n",
      "---------\n",
      "Diseases such as Creutzfeldt-Jakob disease and bovine spongiform encephalopathy (BSE), or mad cow disease, are caused by ________.\n",
      "---------\n",
      "Vaccines protect us against dangerous viral diseases by training the body to recognize and destroy specific invading viruses. Your answer: weakened or dead versions of a dangerous virus. antibiotics. human white blood cells. medicines that cure the symptoms of viral diseases.\n",
      "---------\n",
      "Author Dr. George B. Johnson, Washington University. Author Dr. Peter H. Raven, Missouri Botanical Gardens & Washington University. Contributor Dr. Susan Singer, Carleton College. Contributor Dr. Jonathan Losos, Washington University.\n",
      "---------\n",
      "1.envelope- membrane like structure that surrounds capsid. they sometime steal the membrane from their host cell but they do not function as such. 2. spikes-made of glycoprotien and are associated with enveloped viruses. function as attachment to host cell.\n",
      "---------\n",
      "penetration and uncoating- virus gets into cell and viral genome is released. occur simultaneously for bacteriophage(s) φ-> inject genome (usually all that enters) some φ-> genome inserted into bacteria cell wall for all phage the only thing that enters is genome. animal virus-. (endocytosis) membrane fusion event.\n",
      "---------\n",
      "Examples of viral illnesses range from the common cold, which can be caused by one of the rhinoviruses, to AIDS, which is caused by HIV. Viruses may contain either DNA or RNA as their genetic material. Herpes simplex virus and the hepatitis B virus are DNA viruses. RNA viruses have an enzyme called reverse transcriptase that permits the usual sequence of DNA-to-RNA to be reversed so that the virus can make a DNA version of itself. RNA viruses include HIV and hepatitis C virus.\n",
      "---------\n",
      "Vaccines protect us against dangerous viral diseases by training the body to recognize and destroy specific invading viruses. Vaccines are made from. weakened or dead versions of a dangerous virus. antibiotics. human white blood cells. medicines that cure the symptoms of viral diseases.\n",
      "---------\n",
      "Your answer: a doctor gives a person a measles vaccine. a person becomes immune to chicken pox after contracting it. a person catches a cold. after contracting influenza once, a person can contract it again.\n",
      "**************\n",
      "chemical formula for cuprous oxide\n",
      "Copper oxide or cupric oxide is the inorganic compound with the formula CuO. A black solid, it is one of the two stable oxides of copper, the other being Cu2O or cuprous oxide. As a mineral, it is known as tenorite. It is a product of copper mining and the precursor to many other copper-containing products and chemical compounds.\n",
      "---------\n",
      "The major soluble salts (copper(II) sulfate, copper(II) chloride) are generally more toxic than the less soluble salts (copper(II) hydroxide, copper (II) oxide). Death is preceded by gastric hemorrhage, tachycardia, hypotension, hemolytic crisis, convulsions and paralysis. ...\n",
      "---------\n",
      "Copper(II) oxide can be prepared pyrometallurigically by heating copper metal above 300 deg C in air; preferably 800 deg C is employed. Molten copper is oxidized to copper(II) oxide when sprayed into an oxygen containing gas. Ignition of copper(II) nitrate trihydrate at about 100-200 deg C produces a black oxide. Basic copper(II) carbonate, when heated above 250 deg C, produces a black oxide if a dense carbonate is employed; a brown material is produced when the light and fluffy carbonate is used.\n",
      "---------\n",
      "O=[Cu] Molecular Formula A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols and numbers.\n",
      "---------\n",
      "1 2D Structure A two-dimensional representation of the compound. 2  3D Status Explanation if 3D conformer cannot be generated. Conformer generation is disallowed since MMFF94s unsupported element. 3  Names and Identifiers Record identifiers, synonyms, chemical names, descriptors, etc. Read more...\n",
      "---------\n",
      "Guinea-pigs exposed to copper(II) oxide aerosol at 1.6 mg/cu m... for 1 hour showed significant reductions in tidal volume, minute volume & lung compliance, both during & after exposure, while respiratory frequency was slightly but not significantly incr.\n",
      "---------\n",
      "PubChem uses the Hill system whereby the number of carbon atoms in a molecule is indicated first, the number of hydrogen atoms second, and then the number of all other chemical elements in alphabetical order. When the formula contains no carbon, all the elements, including hydrogen, are listed alphabetically.\n",
      "---------\n",
      "1 2D Structure A two-dimensional representation of the compound. 2  3D Status Explanation if 3D conformer cannot be generated. 3  Names and Identifiers Record identifiers, synonyms, chemical names, descriptors, etc. 4  Chemical and Physical Properties Chemical and physical properties such as melting point, molecular weight, etc.\n",
      "---------\n",
      "Copper(II) oxide. Copper(II) oxide or cupric oxide is the inorganic compound with the formula CuO. A black solid, it is one of the two stable oxides of copper, the other being Cu2O or cuprous oxide. As a mineral, it is known as tenorite and paramelaconite.\n",
      "---------\n",
      "Cupric oxide, or copper (II) oxide, is an inorganic compound with the chemical formula CuO. Cupric oxide is used as a precursor in many copper-containing products such as wood preservatives and ceramics.\n",
      "**************\n",
      "where do mosquito fish come from\n",
      "Aedes albopictus (Stegomyia albopicta), from the mosquito (Culicidae) family, also known as (Asian) tiger mosquito or forest mosquito, is a mosquito native to the tropical and subtropical areas of Southeast Asia; however, in the past few decades, this species has spread to many countries through the transport of goods and international travel.\n",
      "---------\n",
      "taralynna(9)July 21, 2009. This morning I was standing out enjoying my pond and carefully selected koi. A little guppy looking fish swam past me and I freaked out. Could this be a mosquito fish? It has no color except brown/gray. If there is 1 can it have babies? Does there need to be 2 for it to breed? If this is a mosquito ...\n",
      "---------\n",
      "When female mosquitoes drink blood, they purify the blood in their systems and leave a small puddle of urine on their victim's skin. That is why when you first get bit, you may notice a small wet spot surrounding the bite. On average, female mosquitoes drink from 0.001 to 0.1 millimeters of blood per feeding.\n",
      "---------\n",
      "Certain mosquitoes can survive the winter in the larval and pupal stage. All mosquito larvae and pupae require water, even in winter. As the water temperature drops, the mosquito larvae enter a state of diapause, suspending further development and slowing metabolism. Development resumes when the water warms again.\n",
      "---------\n",
      "The western mosquitofish is a species of freshwater fish, also known commonly, if ambiguously, as simply mosquitofish or by its generic name, Gambusia, or by the common name gambezi. There is also an eastern mosquitofish. Mosquitofish are small in comparison to many other freshwater fish, with females reaching an overall length of 7 cm and males at a length of 4 cm. The female can be distinguished from the male by her larger size and a gravid spot at the posterior of her abdomen. The name mosqu\n",
      "---------\n",
      "Mosquitofish in Australia. Mosquitofish. A mosquitofish by the lake in Victoria Park, Sydney. Mosquitofish (Gambusia affinis and Gambusia holbrooki) were introduced to Australia in 1925, spreading from the northeast coasts south to New South Wales, Southern Australia, and parts of Western Australia by 1934.\n",
      "---------\n",
      "Mosquito Fish. Gambusia affins (mosquito fish) The District will deliver fish, at no additional cost, to county residents for control of mosquitoes in backyard ponds. The water in the pond must first be treated to remove chloramine (a sterilizing agent that is present in tap water).\n",
      "---------\n",
      "Mosquitoes spend their first 10 days in water. Water is necessary for the eggs to hatch into larvae, called wigglers. Wigglers feed on organic matter in stagnant water and breathe oxygen from the surface. They develop into pupae, which do not feed and are partially encased in cocoons. Over several days, the pupae change into adult mosquitoes.\n",
      "---------\n",
      "Decreasing the number of native species would also benefit mosquitoes by decreasing the competitive pressure from other fish. Because of their high reproductive rate (an average of 50 young per brood, with up to nine broods per year), fast maturation (sexual maturity is reached in two months), and aggressive behaviour, mosquitofish can outcompete almost any native fish.\n",
      "---------\n",
      "I like the mosquito fish since they, well duh!, keep the mosquito larvae out of the pond, but if they are too much of a nuisance I will have to get rid of a lot more! The water parameters are all fine, and the goldfish behave normally (swimming around) except when I feed them.\n",
      "**************\n",
      "how many banks in the usa\n",
      "MBA in finance. According to the Federal Deposit Insurance Corporation (FDIC) as of March 31, 2014, there were 5,809 commercial banks and 921 savings institutions in the United States whose deposits are insured by the FDIC.\n",
      "---------\n",
      "1 Top 5 Reasons to Choose a Community Bank or Credit Union posted on June 30, 2010. 2  Number of Banks in the U.S., 1966-2014 (Graph) posted on March 23, 2015. 3  Video: Break the Chains, Build Local Power posted on June 1, 2016.  One in Four Local Banks Has Vanished since 2008.\n",
      "---------\n",
      "The number of banks in the U.S. has fallen to a record low this year, with most of the closed institutions consisting of community lenders. With profits harder to come by in financial services and bankers complaining about the burdens of increased regulation, more small lenders are expected to close.\n",
      "---------\n",
      "Access to the Internet is an essential infrastructure for any community that cares about economic development, quality of life, and educational opportunities. Unfortunately, most communities are presently dependent on a few unaccountable absentee corporations that act as gatekeepers to...\n",
      "---------\n",
      "This list includes banks which have failed since October 1, 2000. Failed Bank List - CSV file (Updated on Mondays. Due to the small screen size some information is no longer visible.\n",
      "---------\n",
      "In the 1990s, Congress enacted major changes to our banking policies. These changes untethered banks from their communities, allowed federally insured banks to engage in speculative trading, and fueled a massive wave of mergers.\n",
      "---------\n",
      "This list includes banks which have failed since October 1, 2000. To search for banks that failed prior to those on this page, visit this link: Failures and Assistance Transactions. Failed Bank List - CSV file (Updated on Mondays. Also opens in Excel) Due to the small screen size some information is no longer visible.\n",
      "---------\n",
      "As of year end 1994 there were 10,452 commercial banks and 2,152 savings institutions. According to the Federal Deposit Insurance Corporation (FDIC) as of March 31, 2014, there were 5,809 commercial banks and 921 savings institutions in the United States whose deposits are insured by the FDIC. The number of FDIC insured institutions has declined by almost 50 percent over the past two decades. \n",
      "---------\n",
      "Places that are home to numerous locally owned businesses are more prosperous, sustainable, and resilient than those in which much of the economy is controlled by a few big corporations.\n",
      "---------\n",
      "Wind and sun are available everywhere, so renewable energy can be economically harnessed at small scales across the country. This nature of renewable energy, and the exponential increase of renewable energy generation, promises to decentralize the nation’s grid system. ...\n",
      "**************\n",
      "average protein needs\n",
      "The average man in the Unites States takes in about 100 grams of protein per day, while the average woman takes in approximately 70 grams, according to the National Health and Nutrition Examination Survey. The amount of protein you need daily depends on your total caloric intake, gender and activity level.\n",
      "---------\n",
      "Protein powders and supplements are great for convenience, but are not necessary, even for elite athletic performance. For example, Rossi works with athletes at the University of Virginia and only relies on protein powders when athletes need immediate protein right after a workout and don't have time for a meal.\n",
      "---------\n",
      "While protein is critical in building muscle mass, more is not necessarily better. Simply eating large amounts of lean protein will not equate with a toned body. When determining protein requirements for athletes, it's important to look at the athlete's overall diet.\n",
      "---------\n",
      "Caloric Needs. The average man takes in 2,512 calories per day, according to the survey, while the average woman gets 1,778 calories. Men, who have more muscle mass, a higher caloric intake and larger bodies than women, need more protein each day.\n",
      "---------\n",
      "So how much protein do women need? According to Tara Dellolacono Thies, a registered dietitian and nutritional spokesperson for Clif Bar, most women need between 50 and 60 grams of protein a day.\n",
      "---------\n",
      "Most people with a well-rounded diet eat enough protein, but it's important to include complete proteins, which contain all nine of the essential amino acids. Sources of complete protein include meat, fish, eggs, most dairy products, and soybeans.\n",
      "---------\n",
      "Usual protein intake was calculated on a grams per day, grams per kilogram ideal body weight, and a percentage of calories basis. Protein intake averaged 56 ± 14 g/d in young children, increased to a high of ≈91 ± 22 g/d in adults aged 19–30 y, and decreased to ≈66 ± 17 g/d in the elderly. The percentage of the male population who consumed less than the estimated average requirement was very low.\n",
      "---------\n",
      "Average protein intake on a gram per kilogram ideal body weight basis, the actual DRI format, is presented in Figure 2⇓. Young children 2–3 and 4–8 y of age had average intakes of protein of 4.5 ± 1.1 and 2.4 ± 1.1 g/kg ideal body weight, which is much higher than the EARs of 0.9 and 0.8 g/kg body weight, respectively.\n",
      "---------\n",
      "Eating high-quality protein (such as meat, fish, eggs, dairy or soy) within two hours after exercise — either by itself or with a carbohydrate — enhances muscle repair and growth. Duration and intensity of the activity is also a factor when it comes to protein needs.\n",
      "---------\n",
      "And although meats contain high amounts of protein, be sure to consider how much saturated fat is in your cut. One serving of steak can contain up to 75% of your saturated fat for the day! Here are a few good examples of low-fat, protein-packed foods:\n",
      "**************\n",
      "how is cushing's disease treated\n",
      "The most common cause of Cushing’s syndrome is the use of corticosteroid medications, such as prednisone, in high doses for a long period. Doctors can prescribe these medications to prevent rejection of a transplanted organ. They also use them to treat inflammatory diseases, such as lupus and arthritis.\n",
      "---------\n",
      "By far the majority of dogs diagnosed with Cushings disease result from the over-production of Cortisol in the adrenal glands where the source of the problem is found in the pituitary gland (a gland at the base of the brain bout the size of a pea).\n",
      "---------\n",
      "To find out if you have Cushing's syndrome, a doctor will: 1  Ask questions about the medicines you take, your symptoms, and, if you are a woman, your periods. 2  Take your blood pressure, look for skin changes, and check for changes in your weight and for any signs of cancer.\n",
      "---------\n",
      "Cushing’s syndrome occurs due to abnormally high levels of the hormone cortisol. This can happen for a variety of reasons. The most common cause is overuse of corticosteroid medications. Your doctor may recommend several different diagnostic tests and treatments.\n",
      "---------\n",
      "Adrenal-based hyperadrenocorticism: The adrenal-based form of the disease is usually a result of an adrenal tumor that causes an oversecretion of glucocorticoids. Adrenal tumors are responsible for around 20% of the cases of Cushing's disease.\n",
      "---------\n",
      "Cushings Disease in Dogs. Cushings disease in dogs is the third most common disease and the most common hormone related condition in dogs. It is the result of excess Cortisol in the animal’s body. Cortisol, or hydrocortisone as it is known scientifically, is a steroid hormone that the body uses to deal with stress.\n",
      "---------\n",
      "If you don’t get treatment for it, Cushing’s syndrome can lead to: 1  bone loss. 2  bone fractures. 3  muscle loss and weakness.  high blood 1  pressure. type 2 diabetes. 2  infections. enlargement of a pituitary 3  tumor. kidney  stones.\n",
      "---------\n",
      "A tumor in your pituitary gland that makes extra ACTH, and that causes the adrenal glands to make more cortisol. This is called Cushing's disease. These tumors usually aren't cancer. A tumor in your lung or pancreas that makes ACTH, and that causes the adrenal glands to make more cortisol. Sometimes these tumors are cancer. A tumor in your adrenal glands that makes extra cortisol. Some of these tumors are cancer.\n",
      "---------\n",
      "Cushing's disease is considered a disease of middle age and older dogs and cats. It is much more common in dogs. This disease is similar in cats except that in cats up to 80% also have concurrent diabetes mellitus. This article will refer to the problem as it occurs in dogs.\n",
      "---------\n",
      "Hair Loss and Thin Skin: Hair loss and thinning of the skin are also common symptoms in dogs with Cushing's disease. It is estimated that between 50% and 90% of the affected animals develop these symptoms. Hair loss (alopecia) is one of the most common reasons that owners bring their dog in for evaluation.\n",
      "**************\n",
      "allianz resolution\n",
      "About This Activity. The Allianz Resolution Run 2017 is a 5K road-race that takes place New Years Day at 12 noon in Galway City centre . The course starts at Nimmos Pier and finishes at the Spanish Arch, passing the canal, cathedral, Shop street, Eyre Square and most of the city's famous landmarks along the way. The race is AAI certified.\n",
      "---------\n",
      "NEWS | March 23, 2017. Allianz named ‘International Corporation of the Year’ in Africa. Allianz received the award at the 5th annual Africa CEO Forum held in Geneva on March 20-21. The award recognizes a company for achieving the most outstanding growth of its activities in Africa in a recent period.\n",
      "---------\n",
      "As of 13/04/2016. Holger Tewes-Kampelmann. Managing Director of Resolution Management, Allianz SE Reinsurance (Allianz Re) Personal details. Date of birth: July 28, 1975. Born in Hamm, Germany. German citizenship.\n",
      "---------\n",
      "Disputes and General Insurance Code of Practice. Allianz is a signatory to, and participates in, the General Insurance Code of Practice. This means that we offer an Internal Dispute Resolution Process (IDR) in the event that a customer is not satisfied with either the outcome of a General Insurance claim, or any aspect of it's handling. To start the IDR process, you just have to call us and ask to speak to the Claims Team Leader.\n",
      "---------\n",
      "Fireman’s Fund Insurance Company Information. Claims, support and contact information for Fireman’s Fund policyholders and brokers. Fireman’s Fund Insurance Company is now integrated into Allianz Global Corporate & Specialty (AGCS). This consolidation means that current Fireman’s Fund policyholders and their brokers will benefit from the same focused insurance offerings, but with the added bonus of Allianz’s global reach and infrastructure.\n",
      "---------\n",
      "Claims adjustment in the field of insurance; Consulting and information concerning insurance; Financial administration of reinsurance and insurance actuarial services; Financial advice and consultancy services; Financial risk management; Financial risk management consultation; Insurance actuarial services; Insurance administration; Insurance ...\n",
      "---------\n",
      "Muscles Trump Money in New Year’s Resolutions. MINNEAPOLIS – Dec. 8, 2014 – As Americans make their New Year’s resolutions for 2015, once again, the majority will be more concerned about their waistlines than their wallets, according to the 6th annual New Year’s Resolution Survey* from Allianz Life Insurance Company of North America (Allianz Life).\n",
      "---------\n",
      "December 08, 2014. 6th Annual Allianz Life New Year’s Resolution Survey Finds Renewed Focus on Health and Fitness; Financial Planning Remains a Low Priority.\n",
      "---------\n",
      "This is a brand page for the ALLIANZ RESOLUTION MANAGEMENT trademark by ALLIANZ SE in München, , 80802. Write a review about a product or service associated with this ALLIANZ RESOLUTION MANAGEMENT trademark.\n",
      "---------\n",
      "On Friday, April 24, 2015, a U.S. federal trademark registration was filed for ALLIANZ RESOLUTION MANAGEMENT by ALLIANZ SE, München 80802. The USPTO has given the ALLIANZ RESOLUTION MANAGEMENT trademark serial number of 86608562. The current federal status of this trademark filing is REGISTERED.\n",
      "**************\n",
      "what is thc.\n",
      "THC is short for tetrahydrocannabinol, and it's a chemical compound found in cannabis plants (marijuana). When it's ingested or inhaled, THC binds to the cannabinoid receptors in your brain. Low doses of THC help reduce pain and nausea, and help stimulate the appetite. In some cases, it can also reduce aggression. Larger doses of THC result in the marijuana high - an altered perception of time and space that comes with feelings of happiness, fatigue, or both.\n",
      "---------\n",
      "Effects of marijuana. The nature and effects of the more than 60 cannabinoids present in cannabis are largely unknown, but the most potent psychoactive agent identified is THC. When a person smokes cannabis, THC is quickly absorbed into the bloodstream, reaching the brain within minutes.\n",
      "---------\n",
      "THC is one of more than 60 active ingredients in cannabis. (Photo: Coleen Danger/Trends in Pharmaceutical Sciences). Despite being the most recognized ingredient in marijuana, THC is just one of many compounds in the plant with known medical uses.\n",
      "---------\n",
      "The compound is also known to stimulate appetite (informally known as the munchies) and induce a relaxed state, as well has having effects on the person's sense of smell, hearing and eyesight. THC can also cause fatigue. In some people, THC may reduce symptoms of aggression.\n",
      "---------\n",
      "THC is one of many compounds found in the resin secreted by glands of the marijuana plant. More of these glands are found around the reproductive organs of the plant than on any other area of the plant. Other compounds unique to marijuana, called cannabinoids, are present in this resin.\n",
      "---------\n",
      "THC, or tetrahydrocannabinol, is the primary ingredient in marijuana responsible for the high. But getting high is not all that it’s good for. THC also has a wide range of medical benefits, and is commonly reported to relieve pain, nausea, and depression, among many other things. Scientific research on THC began decades ago in Israel, and has since spread to many countries across the globe.\n",
      "---------\n",
      "6. (Photo: Tinou Bao/Flickr). Most people know of THC because of its ability to induce euphoria, or a high. Interesting enough, THC does not always have this effect on its own. That’s because THC is mostly present in the cannabis plant as THCA (tetrahydrocannabinolic acid), its acidic precursor.\n",
      "---------\n",
      "There are several slang terms for cannabis as a recreational drug, including hash, hashish, grass, pot, weed, and dope. As a recreational drug, cannabis can come in a variety of forms, including: 1  Unprocessed-dried flowers, leaves and stems. 2  Resin. 3  Powder. 4  Oil.\n",
      "---------\n",
      "To understand fully what causes the pleasurable feeling that marijuana is giving, it would be a great help to know what is THC and its properties. THC stands for delta-9-tetrahydrocannabinol, which is the main active chemical in marijuana plant. Marijuana is commonly smoked as a joint or using a pipe. As marijuana is being smoked, THC passes from the lungs going to the bloodstream. From the bloodstream, the chemical THC is carried to the brain and other organs in the body. After a few hours, marijuana users will experience that high feeling. In 1964, THC was first isolated.\n",
      "---------\n",
      "Knowledge center. Marijuana (cannabis) is the most commonly used illicit drug worldwide. Classified as a Schedule 1 controlled substance, marijuana is a mood-altering drug that produces a feeling of calm and well-being. Marijuana also has limited medical use for severe pain relief and other chronic conditions.\n",
      "**************\n",
      "how many bm per day for infant?\n",
      "How many bowel movements should a newborn have in one day? A newborn can have as many as eight to ten bowel movements a day, but as long as she is having at least one, she's probably all right. One day without a bowel movement is usually no cause for concern. As long as your baby is feeding well and wetting her diaper five or six times a day, then she's most likely getting enough to eat.\n",
      "---------\n",
      "After 12 months of age, your child should not take more than 3 portions of milk products per day (one portion of milk = 1 cup or 8 ounces; one portion of yogurt = ¾ cup or 175 grams; one portion of cheese = 50 grams or 1½ ounces). Children between 2 and 8 years need only 2 portions of milk products a day.\n",
      "---------\n",
      "The frequency of bowel movements varies with age. In the first month, babies can have 4-7 stools per day, sometimes after every feed! at about one month, most infants will move their bowels about 1-4 times per day. By about 4 months, some breastfed babies will only need to have a bm once or twice a week! call your doctor if you are not sure. In brief: It depends....\n",
      "---------\n",
      "After about 6 months of age, more than four BMs a day are too many, and less than one a week for a breastfed infant or less than one a day for children over age 2 is too few. That's because we want poop to move through gradually and steadily. If it moves too quickly, the body absorbs less food and nutrition.\n",
      "---------\n",
      "Most babies wait until birth to pass meconium. 1 to 4 months When your baby is 3 or 4 days old, his stool changes from tarry meconium to a watery consistency. Babies usually make up to ten dirty diapers a day for the first one or two months and then go two to four times a day until around 4 months.\n",
      "---------\n",
      "120 Characters remaining. 1  First, try and keep your question as short as possible. 2  Include specific words that will help us identify questions that may already have your answer. 3  If you don't find your answer, you can post your question to WebMD Experts and Contributors.\n",
      "---------\n",
      "Regular bowel movements are important to your child’s health. Bowel habits—how often, how much, and so on—will vary from child to child. Some children go more than once a day, while others may skip a day or two. Eating healthy foods — whole grains, fruits and vegetables — and drinking more water each day. Children should have no more than 120 ml of 100% fruit juice per day.\n",
      "---------\n",
      "How many bowel movements does a 7 month old typically have? My breast fed baby has been on solids for a month: rice cereal w/ breast milk, some organic baby food,& some home made mashed up fruit or veggies. She really enjoys the food. She is still breast fed about 4-5x a day and 2-4x a night. Her bowel movements have drastically dropped from 2 a day to one or two every 2 days.\n",
      "---------\n",
      "If you select Keep me signed in on this computer, you can stay signed in to WebMD.com on this computer for up to 2 weeks or until you sign out. This means that a cookie will stay on your computer even when you exit or close your browser which may reduce your levels of privacy and security. You should never select this option if you're using a publicly accessible computer, or if you're sharing a computer with others.\n",
      "---------\n",
      "How many times a day does a breastfed baby normally poop? What goes in must come out, so just about every feeding in the early weeks following birth should produce a bowel movement. The number and type of movements your baby has will indicate whether he's getting enough to eat. During the meconium phase (the first few days after birth), your baby may have four or five tarry, dark, greenish-black stools spread out over two or three days. As your colostrum develops into mature milk, he should have at least two to five bowel movements in a 24-hour period for the first six weeks.\n",
      "**************\n",
      "how to describe the courses of study\n",
      "course of study - an integrated course of academic studies; he was admitted to a new program at the university. curriculum, syllabus, programme, program. course of lectures - a series of lectures dealing with a subject. info, information - a message received and understood.\n",
      "---------\n",
      "The organizational framework of the BSN program gives direction to the sequencing of courses in the nursing major and allows students to experience concept development in the study of nursing and to meet the overall program objectives.\n",
      "---------\n",
      "This is a work in progress that I assume will take a while to complete. I will be adding links and pages periodically. Course of Study JH/High School: 7th Grade. 8th Grade. 9th Grade. 10th Grade. 11th Grade.\n",
      "---------\n",
      "You will need: Your transcript; Graduation requirements of your high school; Entry requirements for where you want to work or go to school after graduation; Course-of-study worksheet; and Schedule of upcoming courses. Count the number of credits you have completed. Determine how many credits you still need to graduate.\n",
      "---------\n",
      "coursework - work assigned to and done by a student during a course of study; usually it is evaluated as part of the student's grade in the course. adult education - a course (via lectures or correspondence) for adults who are not otherwise engaged in formal study.\n",
      "---------\n",
      "Typical Course of Study Annotated links for the typical course of study for 9th Grade Social Studies to help you cover each topic and check mastery of that skill. Typical Course of Study Annotated links for the typical course of study for 7th Grade Science to help you cover each topic and check mastery of that skill.\n",
      "---------\n",
      "Place all the classes you have taken and passed onto the course-of-study worksheet. You will need a partner for this section of the Lesson. You and your partner will help each other develop a course-of-study to meet your transition visions. You’ll share your course of study with me once you think it is complete.\n",
      "---------\n",
      "Credits may differ from school to school but often: Year long course = 1 credit Semester course = .5 credit Courses that you must take to graduate. Can you give me an example of a required course? Courses that you get to choose. You must take a certain number of electives to graduate.\n",
      "---------\n",
      "Following you'll find the World Book Typical Course of Study for the 12th Grade. 1  Each academic subject is broken down into topics to be covered during 12th Grade. 2  I've taken the topics and found links to sites to help you cover the material and/or check mastery of the skill.\n",
      "---------\n",
      "Bachelor of Science in Nursing Curriculum. The curriculum for the Bachelor of Science in Nursing Degree offered by the Saint Francis Medical Center College of Nursing consists of a total of 124 semester hours of coursework. These hours are divided into 59 semester hours of required pre-nursing courses and 59 semester hours of coursework within the nursing major.\n",
      "**************\n",
      "what is dld service ?\n",
      "DLD Service is distinctive in new pond design and excavation. We are accomplished in. underwater and sensitive area restoration; docks, bridges and waterfalls; levee and dam repair; retaining walls and irrigation. Our specialty is cleaning ponds without draining or drying out the aquatic. area; removing infestations of cat tails and pond shoreline growth.\n",
      "---------\n",
      "All trademarks/service marks referenced on this site are properties of their respective owners. The Acronym Finder is © 1988-2018, Acronym Finder, All Rights Reserved. Feedback\n",
      "---------\n",
      "Quality of Service. Tutorials When running a network, you often have mission-critical services and devices that need to have enough bandwidth available at all times in order to perform reliably.\n",
      "---------\n",
      "What is File Extension DLD? by: Jay Geater, Chief Technology Writer. Did someone email you a DLD file and you're not sure how to open it? Maybe you have found a DLD file on your computer and wondering what it's for? Windows might tell you that you can't open it, or in the worst case, you might encounter a DLD file related error message.\n",
      "---------\n",
      "If you are the owner of a NVIDIA GTX 970, this is worth taking a look at! It seems that NVIDA have settled the class-action lawsuit filed against them for misrepresenting the amount of memory the card really had.\n",
      "---------\n",
      "SearchSecurity. 1  hacker A hacker is an individual who uses computer, networking or other skills to overcome a technical problem. 2  antivirus software (antivirus program) Antivirus software is a class of program designed to prevent, detect and remove malware infections on individual computing ...\n",
      "---------\n",
      "DLD-1 is one of two colorectal adenocarcinoma cell lines which were isolated by D.L. Dexter and associates during a period from 1977-1979.\n",
      "---------\n",
      "It is only a DLD by name, since it is completely unrelated to the 1.4/1.6 units, and is derived from Ford's own 1.8 8v Endura-D engine that saw service through the 1980s and 1990s. However, Ford considers it part of the DLD family, as evidenced by the official DLD name.\n",
      "---------\n",
      "In this phase the design team, testers and customers are plays a major role. Also it should have projects standards, the functional design documents and the database design document also. LLD -- Low Level Design (LLD) is like detailing the HLD. It defines the actual logic for each and every component of the system.\n",
      "---------\n",
      "Incomplete installation of an application that supports the DLD format; The DLD file which is being opened is infected with an undesirable malware. The computer does not have enough hardware resources to cope with the opening of the DLD file. Drivers of equipment used by the computer to open a DLD file are out of date.\n"
     ]
    }
   ],
   "source": [
    "# Printing some queries and answers\n",
    "def printData(query_id):\n",
    "    print('**************')\n",
    "    print(data_dict[query_id]['query']);\n",
    "    print(id2passage[data_dict[query_id]['pos']]);\n",
    "    for i in data_dict[query_id]['negs']:\n",
    "        print('---------')\n",
    "        print(id2passage[i])\n",
    "        \n",
    "for q_id in np.random.choice(range(n_queries), size=20, replace=False):  \n",
    "    printData(q_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNGh0juSq6q8"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "TRAINING_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ECqe-9fcq9Ju",
    "outputId": "79ee641b-d4d6-4f46-bb96-44b85bc378f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUERIES: 419350, VALIDATION EQURIES: 104837\n"
     ]
    }
   ],
   "source": [
    "# divide train and test\n",
    "unique_query_ids = list(data_dict.keys())\n",
    "np.random.shuffle(unique_query_ids) #in-place\n",
    "n_train = int(np.floor(len(unique_query_ids)*TRAINING_PERCENT))\n",
    "n_val = int(np.floor(len(unique_query_ids)*VAL_PERCENT))\n",
    "training_query_ids, validation_query_ids = unique_query_ids[:n_train],unique_query_ids[n_train:n_train+n_val]\n",
    "print('TRAINING QUERIES: {}, VALIDATION EQURIES: {}'.format(n_train,n_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vft5LO05q9mr"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "UNKNOWN_GLOVE = '_unk_';\n",
    "UNKNOWN_ELMO = '';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IX-jpTLBrA9R",
    "outputId": "f269e779-97d9-4e3e-b4a0-6e118c54159e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48640it [00:00, 486395.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400001, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:00, 744847.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtain Glove Embeddings\n",
    "EMB_DIMS = 300;\n",
    "glove_file = r'glove.6B.{}d.txt'.format(EMB_DIMS)\n",
    "#glove_file = r'glove.840B.{}d.txt'.format(EMB_DIMS)\n",
    "#glove_file = r'glove.42B.{}d.txt'.format(EMB_DIMS)\n",
    "dfg = pd.read_csv(glove_file, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "dfg.loc[UNKNOWN_GLOVE]=[0.0]*EMB_DIMS;\n",
    "print(dfg.shape)\n",
    "\n",
    "# make a hash dict\n",
    "word2id = {};\n",
    "id2word = {};\n",
    "for i, word in tqdm(enumerate(dfg.index.values)):\n",
    "  word2id[word] = i;\n",
    "  id2word[i] = word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iKkfYSdPHM0"
   },
   "source": [
    "# Rough Work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zu435s6M-E5b"
   },
   "outputs": [],
   "source": [
    "str_ = \"When I lived in Michigan, 1 child at a daycare I believe was around $150.00 per week (divide that by 40 hours and that comes out to about $3.75 and hour). Where I live now, 1 child at a daycare is $80.00 per week ~ which is about $2.00 an hour. I guess the cost depends on the location.11-13-2007, 09:24 AM. memoriesbre. Location: Tunkhannock. 934 posts, read. times. Reputation: 322. I am in Tunkhannock PA and charge $4.00 an hour and take care of infants in my home. am obviously getting the deal of the century here...I pay $3.25/hr per/kid for after care. So that's not that much I guess. I pay my friends daughter (she's 15) once in a while to sit for me and give her $7.00 an hour.\"\n",
    "cleanText(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUkAOao-rDUk"
   },
   "outputs": [],
   "source": [
    "# Rough Work\n",
    "#print( np.dot(dfg.loc['u.s.a.'],dfg.loc['u.s.']) )\n",
    "#print( np.dot(dfg.loc['u.s.a.'],dfg.loc['inr']) )\n",
    "dfg.loc['u.s.a.'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwJ6KoTarH_g"
   },
   "outputs": [],
   "source": [
    "re.split('\\W+', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QavYlf9uPPUV"
   },
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNU-AnZcrMXh"
   },
   "outputs": [],
   "source": [
    "re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", 'Hello 2018, the popul@tion is 12,000 per 2000 sq. feet')\n",
    "TextBlob('Hello Merands William AWhile 2018 15mw the sence logik popultion ortunately is 12000 per ur mei 2000 sq feet').correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dFGJNp_ratA"
   },
   "source": [
    "# Define Utility funcs and directory to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aniG_nA3rmnH"
   },
   "outputs": [],
   "source": [
    "# Utility Funcs\n",
    "def make_glove_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  glove_arr, glove_bmasks, n_tokens = [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_glove = [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_glove = cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_glove = cstrg.copy(); \n",
    "      cstrg_glove+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per glove requirement\n",
    "    cstrg_glove = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg_glove];\n",
    "    bmask_glove = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg_glove];\n",
    "    cstrg_glove = [word2id[word] for word in cstrg_glove];\n",
    "    glove_arr.append(cstrg_glove);\n",
    "    glove_bmasks.append(bmask_glove);\n",
    "  return np.vstack(glove_arr), np.vstack(glove_bmasks), n_tokens\n",
    "\n",
    "def make_elmo_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  elmo_arr, elmo_bmasks, n_tokens = [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_elmo = [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_elmo = cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_elmo = cstrg.copy(); \n",
    "      cstrg_elmo+=[UNKNOWN_ELMO]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per elmo requirement\n",
    "    elmo_arr.append(cstrg_elmo);\n",
    "    bmask_elmo = [0.0 if word==UNKNOWN_ELMO else 1.0 for word in cstrg_elmo];\n",
    "    elmo_bmasks.append(bmask_elmo)\n",
    "  return elmo_arr, elmo_bmasks, n_tokens\n",
    "\n",
    "\n",
    "def make_glove_and_elmo_batch_data(batch, infer_max_len=True, max_len=0): # batch is list of sentence/paragraph string\n",
    "  glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = [], [], [], [], [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    cstrg_glove, cstrg_elmo = [], [];    \n",
    "    if cstrg_len_[i]>=max_len:\n",
    "      cstrg_glove, cstrg_elmo = cstrg[:max_len].copy(), cstrg[:max_len].copy();\n",
    "      n_tokens.append(max_len);\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg_glove, cstrg_elmo = cstrg.copy(), cstrg.copy(); \n",
    "      cstrg_glove+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "      cstrg_elmo+=[UNKNOWN_ELMO]*(max_len-cstrg_len_[i])\n",
    "      n_tokens.append(cstrg_len_[i]);\n",
    "    # as per glove requirement\n",
    "    cstrg_glove = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg_glove];\n",
    "    bmask_glove = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg_glove];\n",
    "    cstrg_glove = [word2id[word] for word in cstrg_glove];\n",
    "    glove_arr.append(cstrg_glove);\n",
    "    glove_bmasks.append(bmask_glove);\n",
    "    # as per elmo requirement\n",
    "    elmo_arr.append(cstrg_elmo);\n",
    "    bmask_elmo = [0.0 if word==UNKNOWN_ELMO else 1.0 for word in cstrg_elmo];\n",
    "    elmo_bmasks.append(bmask_elmo);\n",
    "  return glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens\n",
    "  #return np.vstack(glove_arr), np.vstack(glove_bmasks), np.vstack(elmo_arr), np.vstack(elmo_bmasks), n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evEnVCATnvOG"
   },
   "source": [
    "# HyperQA Training from Scrath with Glove\n",
    "\n",
    "https://arxiv.org/pdf/1707.07847.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3h-kqCe13PG"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Notes\n",
    "# Different queries have same query id. Ex: 7977\n",
    "# Some queries donot have right answers. Ex: 280200\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W78o2OWf_WqG"
   },
   "outputs": [],
   "source": [
    "# Utility Funcs\n",
    "def make_glove_batch_data(batch, infer_max_len=False, max_len=0): # batch is list of sentence/paragraph string\n",
    "  arr = [];\n",
    "  b_masks = [];\n",
    "  cstrg_, cstrg_len_ = [], [];\n",
    "  for row in batch:\n",
    "    cstrg__, cstrg_len__ = cleanText(row);\n",
    "    cstrg_.append(cstrg__);\n",
    "    cstrg_len_.append(cstrg_len__);\n",
    "  max_len = np.max(cstrg_len_) if infer_max_len else max_len;\n",
    "  for i, cstrg in enumerate(cstrg_):\n",
    "    if cstrg_len_[i]>max_len:\n",
    "      cstrg = cstrg[:max_len];\n",
    "    elif cstrg_len_[i]<max_len:\n",
    "      cstrg+=[UNKNOWN_GLOVE]*(max_len-cstrg_len_[i])\n",
    "    cstrg = [word if word in word2id.keys() else UNKNOWN_GLOVE for word in cstrg];\n",
    "    bmask = [0.0 if word==UNKNOWN_GLOVE else 1.0 for word in cstrg];\n",
    "    cstrg = [word2id[word] for word in cstrg];\n",
    "    arr.append(cstrg);\n",
    "    b_masks.append(bmask);\n",
    "  return np.vstack(arr), np.vstack(b_masks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ8BmSbxKsNb"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA(object):\n",
    "    def __init__(self, vocab_embedding, emb_dim, projection_dim, margin, lr=0.001):\n",
    "        # parameters\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.emb_dim = emb_dim;\n",
    "        # placeholder\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        self.query = tf.placeholder(tf.int32, [None, None])\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        # embedding lookup\n",
    "        self.embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='embeddings')\n",
    "        #self.embedding = tf.get_variable('embeddings', shape=[400001,self.emb_dim], initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.query_emb = tf.nn.embedding_lookup(self.embedding, self.query) # [None,None,emb_dim]\n",
    "        self.pos_ans_emb = tf.nn.embedding_lookup(self.embedding, self.pos_ans) # [None,None,emb_dim]\n",
    "        self.neg_ans_emb = tf.nn.embedding_lookup(self.embedding, self.neg_ans) # [None,None,emb_dim]\n",
    "        # weight vars # a shared variable\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.emb_dim,self.projection_dim],\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=tf.contrib.layers.xavier_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        # projections with input # [None,None,emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)       \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,emb_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.emb_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZpiKMwlFeHH"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 200;\n",
    "\n",
    "# args\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "TRAINING_BATCH_SIZE = 64;\n",
    "VALIDATION_BATCH_SIZE = 1024;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "hyperQA_Graph = tf.Graph();\n",
    "with hyperQA_Graph.as_default():\n",
    "  hyperQA = HyperQA(dfg.as_matrix(), EMB_DIMS, PROJ_DIMS, MARGIN, LEARNING_RATE)\n",
    "  print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWRs1_9HtVxX"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0.5;\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = False;\n",
    "START_EPOCH = 0;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(graph=hyperQA_Graph, config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        dummy_batch_queries = [];\n",
    "        dummy_batch_neg = [];\n",
    "        dummy_batch_neg_n = [];\n",
    "        for query_id in batch_query_ids:\n",
    "          dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "          for jj in data_dict[query_id]['negs']:\n",
    "            dummy_batch_neg.append(id2passage[jj]);\n",
    "          dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))\n",
    "        d_x, d_x1 = make_glove_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "        d_z, d_z1 = make_glove_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "        d_x_new, d_x1_new = [], [];\n",
    "        for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "          d_x_new.append(np.tile(d_x[ii],(n_times,1)));\n",
    "          d_x1_new.append(np.tile(d_x1[ii],(n_times,1)));\n",
    "        d_x_new, d_x1_new = np.vstack(d_x_new), np.vstack(d_x1_new);\n",
    "        result = sess.run([hyperQA.n_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                       hyperQA.query:d_x_new,\n",
    "                                                       hyperQA.query_bmask:d_x1_new,\n",
    "                                                       hyperQA.neg_ans:d_z,\n",
    "                                                       hyperQA.neg_ans_bmask:d_z1})\n",
    "        result = result[0]; # array of batch size i.e. (BS,1)\n",
    "        k=0;\n",
    "        for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "          batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "          k+=n_times;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      y, y1 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "      z, z1 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA.train_op, hyperQA.print_losses, hyperQA.print_p_score_loss, hyperQA.print_n_score_loss],\n",
    "                        feed_dict={hyperQA.keep_prob: 0.8,\n",
    "                                   hyperQA.reg_lambda: 0.00001,\n",
    "                                   hyperQA.query:x,\n",
    "                                   hyperQA.query_bmask:x1,\n",
    "                                   hyperQA.pos_ans:y,\n",
    "                                   hyperQA.pos_ans_bmask:y1,\n",
    "                                   hyperQA.neg_ans:z,\n",
    "                                   hyperQA.neg_ans_bmask:z1})\n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "          dummy_batch_queries = [];\n",
    "          dummy_batch_neg = [];\n",
    "          dummy_batch_neg_n = [];\n",
    "          for query_id in batch_query_ids:\n",
    "            dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "            for jj in data_dict[query_id]['negs']:\n",
    "              dummy_batch_neg.append(id2passage[jj]);\n",
    "            dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))\n",
    "          d_x, d_x1 = make_glove_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "          d_z, d_z1 = make_glove_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "          d_x_new, d_x1_new = [], [];\n",
    "          for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "            d_x_new.append(np.tile(d_x[ii],(n_times,1)));\n",
    "            d_x1_new.append(np.tile(d_x1[ii],(n_times,1)));\n",
    "          d_x_new, d_x1_new = np.vstack(d_x_new), np.vstack(d_x1_new);\n",
    "          result = sess.run([hyperQA.n_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                         hyperQA.query:d_x_new,\n",
    "                                                         hyperQA.query_bmask:d_x1_new,\n",
    "                                                         hyperQA.neg_ans:d_z,\n",
    "                                                         hyperQA.neg_ans_bmask:d_z1})\n",
    "          result = result[0]; # array of batch size i.e. (BS,1)\n",
    "          k=0;\n",
    "          for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "            batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "            k+=n_times;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "        y, y1 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "        z, z1 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA.print_losses, hyperQA.print_p_score_loss, hyperQA.print_n_score_loss],\n",
    "                          feed_dict={hyperQA.keep_prob:1.0,\n",
    "                                     hyperQA.reg_lambda:0.0,\n",
    "                                     hyperQA.query:x,\n",
    "                                     hyperQA.query_bmask:x1,\n",
    "                                     hyperQA.pos_ans:y,\n",
    "                                     hyperQA.pos_ans_bmask:y1,\n",
    "                                     hyperQA.neg_ans:z,\n",
    "                                     hyperQA.neg_ans_bmask:z1})\n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4Z4KCIOs7Oe"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 1024;\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGrf60Up1P1O"
   },
   "outputs": [],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(graph=hyperQA_Graph, config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "      batch_ql_max = -1;\n",
    "      batch_pl_max = -1;\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True);\n",
    "      y, y1 = make_glove_batch_data(batch_passages,infer_max_len=True);\n",
    "      result = sess.run([hyperQA.p_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                     hyperQA.query:x,\n",
    "                                                     hyperQA.query_bmask:x1,\n",
    "                                                     hyperQA.pos_ans:y,\n",
    "                                                     hyperQA.pos_ans_bmask:y1})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "      batch_ql_max = -1;\n",
    "      batch_pl_max = -1;\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    x, x1 = make_glove_batch_data(batch_queries,infer_max_len=True);\n",
    "    y, y1 = make_glove_batch_data(batch_passages,infer_max_len=True);\n",
    "    result = sess.run([hyperQA.p_score],feed_dict={hyperQA.keep_prob: 1.0,\n",
    "                                                   hyperQA.query:x,\n",
    "                                                   hyperQA.query_bmask:x1,\n",
    "                                                   hyperQA.pos_ans:y,\n",
    "                                                   hyperQA.pos_ans_bmask:y1})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "    batch_ql_max = -1;\n",
    "    batch_pl_max = -1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRLARtNC1WnI"
   },
   "outputs": [],
   "source": [
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kivt4Ump1ZUL"
   },
   "outputs": [],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T70ZlM4LTnx"
   },
   "source": [
    "# HyperQA Training from Scrath with Glove and ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir hyperQA_elmo_100\n",
    "model_save_folder = r'hyperQA_elmo_100'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNWGQgpj3AgS"
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\n",
    "# https://tfhub.dev/google/elmo/2\n",
    "# https://tfhub.dev/google/universal-sentence-encoder/2\n",
    "# https://github.com/PrashantRanjan09/WordEmbeddings-Elmo-Fasttext-Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8mmwRu-1kN7"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag_zX_gO1p-V"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA_ELMo(object):\n",
    "    def __init__(self, vocab_embedding, glove_emb_dim, projection_dim, margin, lr=0.001, elmo=None, elmo_emb_dim=1024):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.glove_embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='glove_embedding')\n",
    "        self.glove_emb_dim = glove_emb_dim;\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "        self.elmo = elmo;\n",
    "        self.elmo_emb_dim = elmo_emb_dim;\n",
    "        self.concat_dim = self.glove_emb_dim+self.elmo_emb_dim;\n",
    "        self.init_name = tf.contrib.layers.xavier_initializer()\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.concat_dim,self.projection_dim], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: glove\n",
    "        self.query_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        # placeholder: elmo\n",
    "        self.query_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.query_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.pos_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.neg_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "\n",
    "\n",
    "        # get glove embedding # [None,None,glove_emb_dim]\n",
    "        self.query_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.query_glove)\n",
    "        self.pos_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.pos_ans_glove)\n",
    "        self.neg_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.neg_ans_glove) \n",
    "\n",
    "        # get elmo embeddings # [None,None,elmo_emb_dim]\n",
    "        self.query_elmo_emb = self.ElmoEmbedding(self.query_elmo, self.query_elmo_tokens);\n",
    "        self.pos_ans_elmo_emb = self.ElmoEmbedding(self.pos_ans_elmo, self.pos_ans_elmo_tokens);\n",
    "        self.neg_ans_elmo_emb = self.ElmoEmbedding(self.neg_ans_elmo, self.neg_ans_elmo_tokens);\n",
    "\n",
    "        # concat embeddings # [None,None,concat_dim]\n",
    "        self.query_emb = tf.concat([self.query_glove_emb,self.query_elmo_emb],axis=-1)\n",
    "        self.pos_ans_emb = tf.concat([self.pos_ans_glove_emb,self.pos_ans_elmo_emb],axis=-1)\n",
    "        self.neg_ans_emb = tf.concat([self.neg_ans_glove_emb,self.neg_ans_elmo_emb],axis=-1)\n",
    "\n",
    "        # projections with input # [None,None,glove_emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)  \n",
    "\n",
    "    def ElmoEmbedding(self, tokens_input, tokens_length): #inputs here are tensors \n",
    "        return self.elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]     \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,concat_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.concat_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pynZt8KDBA4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:tensorflow:From <ipython-input-21-c23d89993e18>:106: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "graph built\n"
     ]
    }
   ],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 100;\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 32;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"elmo2\", trainable=True)\n",
    "hyperQA_ELMo = HyperQA_ELMo(dfg.values, EMB_DIMS, PROJ_DIMS, MARGIN, LEARNING_RATE, elmo=elmo, elmo_emb_dim=1024)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMwH-tKh1sdn"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = True;\n",
    "START_EPOCH = 1;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt'.format(epoch));\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "            aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "              aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc1, _, cc3, cc4, cc5 = make_glove_and_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.train_op, hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                        feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                   hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                   hyperQA_ELMo.query_glove:aa1,\n",
    "                                   hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                   hyperQA_ELMo.neg_ans_glove:cc1,\n",
    "                                   hyperQA_ELMo.query_elmo:aa3,\n",
    "                                   hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                   hyperQA_ELMo.query_bmask:aa4,\n",
    "                                   hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                   hyperQA_ELMo.neg_ans_bmask:cc4})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "                #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "                aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "                  aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                    hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                    hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                    hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                    hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc1, _, cc3, cc4, cc5 = make_glove_and_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                          feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                     hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                     hyperQA_ELMo.query_glove:aa1,\n",
    "                                     hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                     hyperQA_ELMo.neg_ans_glove:cc1,\n",
    "                                     hyperQA_ELMo.query_elmo:aa3,\n",
    "                                     hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                     hyperQA_ELMo.query_bmask:aa4,\n",
    "                                     hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                     hyperQA_ELMo.neg_ans_bmask:cc4})  \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r879ahcT2CE4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"elmo2\", trainable=True)\n",
    "#def ElmoEmbedding(x_tf): #x_tf is a tensor \n",
    "#    return elmo(x_tf, as_dict = True, signature='default')['elmo']\n",
    "def ElmoEmbedding(tokens_input, tokens_length): #inputs here are tensors \n",
    "    return elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]\n",
    "    \n",
    "#embeddings = elmo([\"the cat is on the mat\", \"dogs are in the fog\"],signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "#embeddings\n",
    "\n",
    "test_batch = ['How are you doing?','What is your age?', 'How to kill a cockroach..damn'];\n",
    "glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch) \n",
    "for item in [glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens]:\n",
    "    print(item)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPuDks6o2HB-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "glove_inp = tf.placeholder(tf.int32, shape=(None, None))\n",
    "elmo_inp = tf.placeholder(tf.string, shape=(None, None))\n",
    "elmo_token_len = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "embedding_tf = tf.Variable(dfg.values , trainable=False, dtype=tf.float32, name='embeddings')\n",
    "glove_emb = tf.nn.embedding_lookup(embedding_tf, glove_inp) # [None,None,emb_dim]\n",
    "elmo_emb = ElmoEmbedding(elmo_inp, elmo_token_len);\n",
    "final_emb = tf.concat([glove_emb,elmo_emb],axis=-1,name='concat')\n",
    "\n",
    "sess = tf.Session(config=tf_sess_config);\n",
    "sess.run(tf.global_variables_initializer());\n",
    "sess.run(tf.tables_initializer());\n",
    "check = sess.run([final_emb,glove_emb,elmo_emb],feed_dict={glove_inp:glove_arr,elmo_inp:elmo_arr,elmo_token_len:n_tokens})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSJ8KNBF2KAA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 32;\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0acJVWw-2NCy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./hyperQA_elmo_100/epoch_model_1.ckpt\n",
      "Percent: [------------------->][104169/104170] 100% ||  : -1.000000 | "
     ]
    }
   ],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_1.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                          hyperQA_ELMo.query_glove:aa1,\n",
    "                                                          hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                          hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                          hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                          hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                          hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                        hyperQA_ELMo.query_glove:aa1,\n",
    "                                                        hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                        hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                        hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                        hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                        hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4D8bl3ob2PQ6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104170it [00:07, 13759.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67CAw6pZ_Rnm"
   },
   "source": [
    "# HyperQA Training from Scrath with ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfMsjGFX_U6r"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDtXL_Eh_itd"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class HyperQA_ELMo(object):\n",
    "    def __init__(self, projection_dim, margin, lr=0.001, elmo=None, elmo_emb_dim=1024):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.projection_dim = projection_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "        self.elmo = elmo;\n",
    "        self.elmo_emb_dim = self.concat_dim = elmo_emb_dim;\n",
    "        self.init_name = tf.contrib.layers.xavier_initializer()\n",
    "        self.w_p = tf.get_variable(\"w_p\", shape=[self.concat_dim,self.projection_dim], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_p = tf.get_variable('b_p', shape=[self.projection_dim], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "        self.w_f = tf.get_variable(\"w_f\", shape=[1], initializer=self.init_name, trainable=True, dtype=tf.float32)\n",
    "        self.b_f = tf.get_variable('b_f', shape=[1], initializer=tf.zeros_initializer(), trainable=True, dtype=tf.float32)\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: elmo\n",
    "        self.query_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.query_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.pos_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_elmo = tf.placeholder(tf.string, shape=(None, None));\n",
    "        self.neg_ans_elmo_tokens = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "\n",
    "        # get elmo embeddings # [None,None,elmo_emb_dim]\n",
    "        self.query_elmo_emb = self.ElmoEmbedding(self.query_elmo, self.query_elmo_tokens);\n",
    "        self.pos_ans_elmo_emb = self.ElmoEmbedding(self.pos_ans_elmo, self.pos_ans_elmo_tokens);\n",
    "        self.neg_ans_elmo_emb = self.ElmoEmbedding(self.neg_ans_elmo, self.neg_ans_elmo_tokens);\n",
    "\n",
    "        # concat embeddings # [None,None,concat_dim]\n",
    "        self.query_emb = self.query_elmo_emb\n",
    "        self.pos_ans_emb = self.pos_ans_elmo_emb\n",
    "        self.neg_ans_emb = self.neg_ans_elmo_emb\n",
    "\n",
    "        # projections with input # [None,None,glove_emb_dim] and output # [None,None,proj_emb]\n",
    "        self.query_proj_ = self.project_fn(self.query_emb, self.query_bmask);\n",
    "        self.pos_ans_proj_ = self.project_fn(self.pos_ans_emb, self.pos_ans_bmask);\n",
    "        self.neg_ans_proj_ = self.project_fn(self.neg_ans_emb, self.neg_ans_bmask);\n",
    "        # dropout\n",
    "        self.query_proj = tf.nn.dropout(self.query_proj_, self.keep_prob);\n",
    "        self.pos_ans_proj = tf.nn.dropout(self.pos_ans_proj_, self.keep_prob);\n",
    "        self.neg_ans_proj = tf.nn.dropout(self.neg_ans_proj_, self.keep_prob);\n",
    "        # unit normalized representations with output #[None,proj_emb]\n",
    "        self.query_vec = tf.clip_by_norm(tf.reduce_sum(self.query_proj,axis=1), 1.0, axes=1) \n",
    "        self.pos_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.pos_ans_proj,axis=1), 1.0, axes=1)\n",
    "        self.neg_ans_vec = tf.clip_by_norm(tf.reduce_sum(self.neg_ans_proj,axis=1), 1.0, axes=1)\n",
    "        # hyperbolic distance\n",
    "        self.p_distance = self.hyperbolic_ball(self.query_vec, self.pos_ans_vec) #[None,1]\n",
    "        self.n_distance = self.hyperbolic_ball(self.query_vec, self.neg_ans_vec) #[None,1]\n",
    "        # loss\n",
    "        self.p_score = self.p_distance*self.w_f+self.b_f; \n",
    "        self.n_score = self.n_distance*self.w_f+self.b_f;\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = self.reg_losses+tf.reduce_sum(self.losses) #[]\n",
    "        # print loss ops\n",
    "        self.print_p_distance = tf.reduce_mean(self.p_distance)\n",
    "        self.print_n_distance = tf.reduce_mean(self.n_distance)\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        reim_gradients = [(self._to_riemannian_gradient(grad), var) for grad, var in gradients]\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in reim_gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)  \n",
    "\n",
    "    def ElmoEmbedding(self, tokens_input, tokens_length): #inputs here are tensors \n",
    "        return self.elmo(inputs={\"tokens\": tokens_input,\"sequence_len\": tokens_length},signature=\"tokens\",as_dict=True)[\"elmo\"]     \n",
    "    def project_fn(self, inp_emb, bmask): # Input Shape [None,None,concat_dim], [None,None]\n",
    "        runtime_shape = tf.shape(inp_emb);\n",
    "        dim1 = runtime_shape[0];\n",
    "        dim2 = runtime_shape[1];\n",
    "        dense_output = tf.nn.xw_plus_b(tf.reshape(inp_emb, [-1,self.concat_dim]), self.w_p, self.b_p);\n",
    "        activated_output = tf.nn.relu(dense_output);\n",
    "        proj_emb = tf.reshape(activated_output,[dim1,dim2,self.projection_dim]);\n",
    "        bmask = tf.tile(tf.expand_dims(bmask,axis=-1),[1,1,self.projection_dim]);\n",
    "        masked_proj_emb = bmask*proj_emb;\n",
    "        return masked_proj_emb\n",
    "    def hyperbolic_ball(self, x, y, neg=False, eps=1E-6):\n",
    "        \"\"\" Poincare Distance Function \"\"\"\n",
    "        z = x - y\n",
    "        z = tf.norm(z, ord='euclidean', keep_dims=True, axis=1)\n",
    "        z = tf.square(z)\n",
    "        x_d = 1 - tf.square(tf.norm(x, ord='euclidean', keep_dims=True, axis=1))\n",
    "        y_d = 1 - tf.square(tf.norm(y, ord='euclidean', keep_dims=True, axis=1))\n",
    "        d = x_d * y_d\n",
    "        z = z / (d + eps)\n",
    "        z  = (2 * z) + 1\n",
    "        arcosh = z + tf.sqrt(tf.square(z) - 1 + eps)\n",
    "        arcosh = tf.log(arcosh)\n",
    "        if(neg):\n",
    "            arcosh = -arcosh\n",
    "        return arcosh\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        #grad = tf.clip_by_norm(grad, 1.0)\n",
    "        return grad\n",
    "    def _to_riemannian_gradient(self, ge):\n",
    "      if ge is None:\n",
    "        return None\n",
    "      try:\n",
    "        shape = ge.get_shape().as_list()\n",
    "        if len(shape) >= 3:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, axis=[-2, -1], keepdims=True))\n",
    "        elif len(shape) == 2:\n",
    "            grad_scale = 1 - tf.square(tf.norm(ge, keepdims=True))\n",
    "        else:\n",
    "            return ge\n",
    "      except:\n",
    "        print('Exception handled!')\n",
    "        grad_scale = 1 - tf.square(tf.norm(ge, keep_dims=True))\n",
    "      grad_scale = (tf.square(grad_scale) + 1e-10) / 4.0\n",
    "      gr = ge * grad_scale\n",
    "      # gr = tf.clip_by_norm(gr, 1.0, axes=0)\n",
    "      return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGY5rJnLBmWu"
   },
   "outputs": [],
   "source": [
    "# some global vars\n",
    "PROJ_DIMS = 100;\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 32;\n",
    "VALIDATION_EVERY = 1;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "hyperQA_ELMo = HyperQA_ELMo(PROJ_DIMS, MARGIN, LEARNING_RATE, elmo=elmo, elmo_emb_dim=1024)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEILTye8DxKo"
   },
   "outputs": [],
   "source": [
    "cleanText = cleanText_Spaces_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_zpgJKw__o7"
   },
   "outputs": [],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = False;\n",
    "START_EPOCH = 0;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session() as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt'.format(epoch));\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "            aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "              aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa3, aa4, aa5 = make_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb3, bb4, bb5 = make_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc3, cc4, cc5 = make_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.train_op, hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                        feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                   hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                   hyperQA_ELMo.query_elmo:aa3,\n",
    "                                   hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                   hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                   hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                   hyperQA_ELMo.query_bmask:aa4,\n",
    "                                   hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                   hyperQA_ELMo.neg_ans_bmask:cc4})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "                #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "                aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "                  aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([hyperQA_ELMo.n_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                                    hyperQA_ELMo.query_glove:aa1_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo:aa3_new,\n",
    "                                                                    hyperQA_ELMo.query_bmask:aa4_new,\n",
    "                                                                    hyperQA_ELMo.query_elmo_tokens:aa5_new,\n",
    "                                                                    hyperQA_ELMo.neg_ans_glove:bb1,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo:bb3,\n",
    "                                                                    hyperQA_ELMo.neg_ans_bmask:bb4,\n",
    "                                                                    hyperQA_ELMo.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc3, cc4, cc5 = make_glove_and_elmo_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([hyperQA_ELMo.train_op, hyperQA_ELMo.print_losses, hyperQA_ELMo.print_p_score_loss, hyperQA_ELMo.print_n_score_loss],\n",
    "                          feed_dict={hyperQA_ELMo.keep_prob: 0.8,\n",
    "                                     hyperQA_ELMo.reg_lambda: 0.00001,\n",
    "                                     hyperQA_ELMo.query_elmo:aa3,\n",
    "                                     hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                     hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo:cc3,\n",
    "                                     hyperQA_ELMo.neg_ans_elmo_tokens:cc5,\n",
    "                                     hyperQA_ELMo.query_bmask:aa4,\n",
    "                                     hyperQA_ELMo.pos_ans_bmask:bb4,\n",
    "                                     hyperQA_ELMo.neg_ans_bmask:cc4})  \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2sFyjGb6EqU"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 32;\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBtqGHYl6Jy0"
   },
   "outputs": [],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_0.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                          hyperQA_ELMo.query_glove:aa1,\n",
    "                                                          hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                          hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                          hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                          hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                          hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                          hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([hyperQA_ELMo.p_score],feed_dict={hyperQA_ELMo.keep_prob: 1.0,\n",
    "                                                        hyperQA_ELMo.query_glove:aa1,\n",
    "                                                        hyperQA_ELMo.pos_ans_glove:bb1,\n",
    "                                                        hyperQA_ELMo.query_elmo:aa3,\n",
    "                                                        hyperQA_ELMo.query_elmo_tokens:aa5,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo:bb3,\n",
    "                                                        hyperQA_ELMo.pos_ans_elmo_tokens:bb5,\n",
    "                                                        hyperQA_ELMo.query_bmask:aa4,\n",
    "                                                        hyperQA_ELMo.pos_ans_bmask:bb4})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];\n",
    "outputs = np.vstack(outputs)\n",
    "df_test['pred'] = outputs; # the smaller the score, the more positive_answer it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnZLz7Vp6Q-d"
   },
   "outputs": [],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'answer.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcqYS24w6Xyk"
   },
   "source": [
    "# BiLSTM-Attention-SiameseLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save \n",
    "!mkdir biLSTM_Attention\n",
    "model_save_folder = r'biLSTM_Attention'\n",
    "dfile = open('./'+model_save_folder+'/dummy.txt','w',encoding='utf-8');\n",
    "dfile.write('Checking...');\n",
    "dfile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv8vNj5m6gpy"
   },
   "outputs": [],
   "source": [
    "# HyperQA Tensorflow model\n",
    "\n",
    "class BiLSTM_Attention(object):\n",
    "    def __init__(self, vocab_embedding, glove_emb_dim, margin, lr=0.001):\n",
    "\n",
    "        # parameters and variables\n",
    "        self.glove_embedding = tf.Variable(vocab_embedding, trainable=False, dtype=tf.float32, name='glove_embedding');\n",
    "        self.glove_emb_dim = glove_emb_dim;\n",
    "        self.margin = margin;\n",
    "        self.lr = lr;\n",
    "\n",
    "        # placeholder: general\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.reg_lambda = tf.placeholder(tf.float32, [])\n",
    "        # placeholder: glove\n",
    "        self.query_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.pos_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        self.neg_ans_glove = tf.placeholder(tf.int32, [None, None])\n",
    "        # placeholder: tokens\n",
    "        self.query_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.pos_ans_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        self.neg_ans_glove_seq_lengths = tf.placeholder(tf.int32, shape=(None));\n",
    "        # placeholder: masking\n",
    "        #self.query_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        #self.pos_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        #self.neg_ans_bmask = tf.placeholder(tf.float32, [None, None])\n",
    "        \n",
    "        # layer: glove embedding # [None,None,glove_emb_dim]\n",
    "        self.query_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.query_glove)\n",
    "        self.pos_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.pos_ans_glove)\n",
    "        self.neg_ans_glove_emb = tf.nn.embedding_lookup(self.glove_embedding, self.neg_ans_glove)\n",
    "        \n",
    "        # layer: BiLSTM with 2 layers # time_major=False # [None,None,cell_fw.ouput_size+cell_bw.output_size]\n",
    "        with tf.variable_scope(\"query_BiLSTM\"):\n",
    "          self.query_BiLSTM = self.query_BiLSTM_fn(self.query_glove_emb,self.query_glove_seq_lengths)\n",
    "        with tf.variable_scope(\"answer_BiLSTM\", reuse = tf.AUTO_REUSE):\n",
    "          self.pos_ans_BiLSTM = self.answer_BiLSTM_fn(self.pos_ans_glove_emb,self.pos_ans_glove_seq_lengths)\n",
    "          self.neg_ans_BiLSTM = self.answer_BiLSTM_fn(self.neg_ans_glove_emb,self.neg_ans_glove_seq_lengths)\n",
    " \n",
    "        # layer: Attention+SiameseEncoder\n",
    "        self.p_score = self.get_score_fn(self.query_BiLSTM, self.query_glove_seq_lengths, self.pos_ans_BiLSTM, self.pos_ans_glove_seq_lengths);\n",
    "        self.n_score = self.get_score_fn(self.query_BiLSTM, self.query_glove_seq_lengths, self.neg_ans_BiLSTM, self.neg_ans_glove_seq_lengths);\n",
    "      \n",
    "        # layer: Loss and Optimization\n",
    "        self.losses = tf.nn.relu(self.margin + self.n_score - self.p_score) #[None,1]\n",
    "        #self.reg_losses = self.reg_lambda*tf.reduce_sum(tf.abs(self.w_p));\n",
    "        self.loss = tf.reduce_sum(self.losses) # +self.reg_losses\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # adjust gradient\n",
    "        gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        clip_gradients = [(self._ClipIfNotNone(grad), var) for grad, var in gradients]\n",
    "        self.train_op = self.optimizer.apply_gradients(clip_gradients)\n",
    "        \n",
    "        # print loss ops\n",
    "        self.print_p_score_loss = tf.reduce_mean(self.p_score)\n",
    "        self.print_n_score_loss = tf.reduce_mean(self.n_score)\n",
    "        self.print_losses = tf.reduce_mean(self.losses)\n",
    "    \n",
    "    # Function Definitions\n",
    "    def query_BiLSTM_fn(self, query_input, query_seq_lengths): # [None,None,glove_emb_dim]\n",
    "      query_outputs_L1, query_output_states_L1 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=query_input,sequence_length=query_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      query_outputs_concat_L1 = tf.concat(query_outputs_L1, 2)\n",
    "      query_outputs_L2, query_output_states_L2 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=query_outputs_concat_L1,sequence_length=query_seq_lengths,dtype=tf.float32,scope='L2')\n",
    "      query_outputs_concat_L2 = tf.concat(query_outputs_L2, 2)\n",
    "      return query_outputs_concat_L2\n",
    "    def answer_BiLSTM_fn(self, answer_input, answer_seq_lengths): # [None,None,glove_emb_dim]\n",
    "      answer_outputs_L1, answer_output_states_L1 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=answer_input,sequence_length=answer_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      answer_outputs_concat_L1 = tf.concat(answer_outputs_L1, 2)\n",
    "      answer_outputs_L2, answer_output_states_L2 = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=128),tf.nn.rnn_cell.LSTMCell(num_units=128),inputs=answer_outputs_concat_L1,sequence_length=answer_seq_lengths,dtype=tf.float32,scope='L2')\n",
    "      answer_outputs_concat_L2 = tf.concat(answer_outputs_L2, 2)\n",
    "      return answer_outputs_concat_L2\n",
    "    def attention_fn(self, query, answer): # shape of dim1 in both inputs must be equal. # dim2 are time_axis and can be variable length\n",
    "      query_time_steps, answer_time_steps = tf.shape(query)[1], tf.shape(answer)[1];\n",
    "      # attention scores\n",
    "      interaction_matrix = tf.matmul(query, tf.transpose(answer, perm=[0, 2, 1])); # shape [inputs_1.dims1, inputs_1.dims2, inputs_2.dims2]\n",
    "      query_softmax_, answer_softmax_ = tf.nn.softmax(interaction_matrix,axis=2), tf.nn.softmax(interaction_matrix,axis=1)\n",
    "      query_softmax, answer_softmax = tf.expand_dims(query_softmax_, axis=-1), tf.expand_dims(answer_softmax_, axis=-1)\n",
    "      # query_attention\n",
    "      query_expanded = tf.tile(tf.expand_dims(query,axis=-2), [1,1,answer_time_steps,1]);\n",
    "      query_attn_ = query_expanded*query_softmax;\n",
    "      query_attn = tf.reduce_sum(query_attn_, axis=-2);\n",
    "      # answer_attention\n",
    "      answer_expanded = tf.tile(tf.expand_dims(answer,axis=-3), [1,query_time_steps,1,1]);\n",
    "      answer_attn_ = answer_expanded*answer_softmax;\n",
    "      answer_attn = tf.reduce_sum(answer_attn_, axis=-3);\n",
    "      # outputs\n",
    "      output_query = tf.concat([query,query_attn], axis=-1);\n",
    "      output_answer = tf.concat([answer,answer_attn], axis=-1);      \n",
    "      return output_query, output_answer\n",
    "    def siamese_BiLSTM_fn(self, input_seq, input_seq_lengths): # [None,None,2*(cell_fw.ouput_size+cell_bw.output_size)]\n",
    "      outputs_L1, _ = tf.nn.bidirectional_dynamic_rnn(tf.nn.rnn_cell.LSTMCell(num_units=64),tf.nn.rnn_cell.LSTMCell(num_units=64),inputs=input_seq,sequence_length=input_seq_lengths,dtype=tf.float32,scope='L1')\n",
    "      outputs_concat_L1 = tf.concat(outputs_L1, 2)\n",
    "      return outputs_concat_L1    \n",
    "    def extract_axis_1(self, data, ind):\n",
    "      # https://stackoverflow.com/questions/41273361/get-the-last-output-of-a-dynamic-rnn-in-tensorflow\n",
    "      batch_range = tf.range(tf.shape(data)[0])\n",
    "      indices = tf.stack([batch_range, ind], axis=1)\n",
    "      res = tf.gather_nd(data, indices)\n",
    "      return res # [batch_size, num_cells]\n",
    "    def get_score_fn(self, query, query_seq_lengths, answer, answer_seq_lengths):\n",
    "      query_with_attn, answer_with_attn = self.attention_fn(query, answer);\n",
    "      with tf.variable_scope(\"siamese_BiLSTM_encoder\", reuse = tf.AUTO_REUSE):\n",
    "        a1 = self.siamese_BiLSTM_fn(query_with_attn, query_seq_lengths);\n",
    "        a2 = self.extract_axis_1(a1, query_seq_lengths-1)\n",
    "        b1 = self.siamese_BiLSTM_fn(answer_with_attn, answer_seq_lengths);\n",
    "        b2 = self.extract_axis_1(b1, answer_seq_lengths-1)\n",
    "        c = tf.reduce_sum(tf.multiply(a2,b2), axis=-1);\n",
    "      return c\n",
    "    def _ClipIfNotNone(self, grad):\n",
    "        if grad is None:\n",
    "          return grad\n",
    "        #grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "        grad = tf.clip_by_norm(grad, 5.0)\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QgV7IcDX6nyv",
    "outputId": "96658bcf-a374-4870-e35e-3eefcba72810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /data/ontology/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "graph built\n"
     ]
    }
   ],
   "source": [
    "# some global vars\n",
    "MARGIN = 5.0;\n",
    "LEARNING_RATE = 0.001;\n",
    "\n",
    "# Create a graph with given params\n",
    "tf.reset_default_graph();\n",
    "biLSTM_Attention = BiLSTM_Attention(dfg.values, EMB_DIMS, MARGIN, LEARNING_RATE)\n",
    "print('graph built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfjH0V8A7xrT"
   },
   "outputs": [],
   "source": [
    "cleanText = cleanText_Simple;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "colab_type": "code",
    "id": "1KfPm76C6uQB",
    "outputId": "bb4edea2-f784-46a3-e699-27b1913c90a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./biLSTM_Attention/epoch_model_1.ckpt\n",
      "beginning epoch: 2\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 2.074297 | POS_LOSS: 17.812391 | NEG_LOSS: 12.196445 | \n",
      "TRAINING || mean loss: 2.9094421681112204 | mean pos loss: 17.57220043986126 | mean neg loss: 13.190940938371442\n",
      "beginning epoch: 3\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 2.470434 | POS_LOSS: 17.258129 | NEG_LOSS: 12.327210 | \n",
      "TRAINING || mean loss: 2.776450111943505 | mean pos loss: 17.001830450271658 | mean neg loss: 12.20958774736076\n",
      "beginning epoch: 4\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 2.100240 | POS_LOSS: 16.264469 | NEG_LOSS: 11.299400 | \n",
      "TRAINING || mean loss: 2.6737185398273633 | mean pos loss: 16.92666070263319 | mean neg loss: 11.808108300188959\n",
      "beginning epoch: 5\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 1.620027 | POS_LOSS: 16.162506 | NEG_LOSS: 9.592369 |  \n",
      "TRAINING || mean loss: 2.5735720345552586 | mean pos loss: 16.211709367704916 | mean neg loss: 10.768043097358774\n",
      "beginning epoch: 6\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 3.271286 | POS_LOSS: 17.455402 | NEG_LOSS: 12.728913 | \n",
      "TRAINING || mean loss: 2.487196451805814 | mean pos loss: 15.322625853421487 | mean neg loss: 9.61128600325631\n",
      "beginning epoch: 7\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 2.212939 | POS_LOSS: 17.704174 | NEG_LOSS: 10.916351 | \n",
      "TRAINING || mean loss: 2.396887103682418 | mean pos loss: 15.998923836464702 | mean neg loss: 9.951515152231677\n",
      "beginning epoch: 8\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 2.879395 | POS_LOSS: 17.702486 | NEG_LOSS: 12.387991 | \n",
      "TRAINING || mean loss: 2.3301038623643047 | mean pos loss: 17.342778343639093 | mean neg loss: 11.058168113122493\n",
      "beginning epoch: 9\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 1.711782 | POS_LOSS: 16.296206 | NEG_LOSS: 8.705193 |  \n",
      "TRAINING || mean loss: 2.2523271609939206 | mean pos loss: 17.15207167801257 | mean neg loss: 10.588735089779483\n",
      "beginning epoch: 10\n",
      "Percent: [------------------->][419328/419350] 100% ||  LOSS: 1.507087 | POS_LOSS: 16.642929 | NEG_LOSS: 8.854691 |  \n",
      "TRAINING || mean loss: 2.188460850151158 | mean pos loss: 17.034145238344195 | mean neg loss: 10.305763201835829\n",
      "Percent: [------------------->][104832/104837] 100% ||  LOSS: 2.581478 | POS_LOSS: 14.257473 | NEG_LOSS: 8.619963 |  \n",
      "VALIDATION || mean loss: 3.14290392267835 | mean pos loss: 15.169930379469315 | mean neg loss: 10.009642561713418\n",
      "beginning epoch: 11\n",
      "Percent: [------->            ][158048/419350] 38% ||  LOSS: 1.658637 | POS_LOSS: 18.778111 | NEG_LOSS: 11.632693 | "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c50b1f299981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m                                    \u001b[0mbiLSTM_Attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_glove_seq_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maa3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                    \u001b[0mbiLSTM_Attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ans_glove_seq_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbb3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                    biLSTM_Attention.neg_ans_glove_seq_lengths:cc3})    \n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0mcum_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mcum_pos_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env_murali/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NEW: max sampling from https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_CIKM2016.pdf\n",
    "# Run this cell for both random_ as well as max_sampling\n",
    "choice_max = 0; # the greater this is, the more cprob to chosse MAX over RANDOM\n",
    "default_sampling_choice = 'RANDOM';\n",
    "RESUME_TRAINING = True;\n",
    "START_EPOCH = 2;\n",
    "TRAINING_BATCH_SIZE = 32;\n",
    "VALIDATION_BATCH_SIZE = 64;\n",
    "VALIDATION_EVERY = 10;\n",
    "N_EPOCHS = 100;\n",
    "\n",
    "# Run the connections for desired epochs\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  if not RESUME_TRAINING:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    START_EPOCH = 0;\n",
    "  else:\n",
    "    saver.restore(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(START_EPOCH-1));\n",
    "  for epoch in np.arange(START_EPOCH, N_EPOCHS):\n",
    "    # sampling choice\n",
    "    if epoch==0:\n",
    "      sampling_choice = default_sampling_choice;\n",
    "    else:\n",
    "      sampling_choice = 'MAX' if np.random.random()<choice_max else default_sampling_choice;\n",
    "    # Loss and optimization on Training Data\n",
    "    print('beginning epoch: {}'.format(epoch));\n",
    "    n_batches = int(np.floor(n_train/TRAINING_BATCH_SIZE));\n",
    "    cum_loss = 0;\n",
    "    cum_pos_loss = 0;\n",
    "    cum_neg_loss = 0;\n",
    "    frm=0;\n",
    "    # TRAIN\n",
    "    for i in range(n_batches):\n",
    "      if i%1000==0:\n",
    "        saver.save(sess, './'+model_save_folder+'/batch_model.ckpt');\n",
    "      batch_query_ids = training_query_ids[frm:frm+TRAINING_BATCH_SIZE];\n",
    "      batch_queries = [];\n",
    "      batch_pos = [];\n",
    "      batch_neg = [];\n",
    "      batch_arg_neg = [];\n",
    "      # TRAIN: Find the most challenging negative!\n",
    "      if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "        unfort_index = 0;\n",
    "        while(unfort_index<len(batch_query_ids)):\n",
    "            small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "            dummy_batch_queries = [];\n",
    "            dummy_batch_neg = [];\n",
    "            dummy_batch_neg_n = [];\n",
    "            for query_id in small_batch_query_ids:\n",
    "              dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "              for jj in data_dict[query_id]['negs']:\n",
    "                dummy_batch_neg.append(id2passage[jj]);\n",
    "              dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "            #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "            aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "            bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "            for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "              aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "              aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "              aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "              aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "            aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "            aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "            result = sess.run([biLSTM_Attention.n_score],feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                                                biLSTM_Attention.query_glove:aa1_new,\n",
    "                                                                biLSTM_Attention.query_elmo:aa3_new,\n",
    "                                                                biLSTM_Attention.query_bmask:aa4_new,\n",
    "                                                                biLSTM_Attention.query_elmo_tokens:aa5_new,\n",
    "                                                                biLSTM_Attention.neg_ans_glove:bb1,\n",
    "                                                                biLSTM_Attention.neg_ans_elmo:bb3,\n",
    "                                                                biLSTM_Attention.neg_ans_bmask:bb4,\n",
    "                                                                biLSTM_Attention.neg_ans_elmo_tokens:bb5})\n",
    "            result = result[0]; # array of batch size i.e. (BS,1)\n",
    "            k=0;\n",
    "            for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "              batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "              k+=n_times;\n",
    "            unfort_index+=3;\n",
    "      # TRAIN: compute loss and optimize\n",
    "      if sampling_choice=='MAX':\n",
    "        assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "      for i_id, query_id in enumerate(batch_query_ids):\n",
    "        batch_queries.append(data_dict[query_id]['query']);\n",
    "        batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "        this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "        batch_neg.append(this_negative);\n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "      aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "      cc1, _, cc3 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "      result = sess.run([biLSTM_Attention.train_op, biLSTM_Attention.print_losses, biLSTM_Attention.print_p_score_loss, biLSTM_Attention.print_n_score_loss],\n",
    "                        feed_dict={biLSTM_Attention.keep_prob: 0.8,\n",
    "                                   biLSTM_Attention.reg_lambda: 0.00001,\n",
    "                                   biLSTM_Attention.query_glove:aa1,\n",
    "                                   biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                   biLSTM_Attention.neg_ans_glove:cc1,\n",
    "                                   biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                   biLSTM_Attention.pos_ans_glove_seq_lengths:bb3,\n",
    "                                   biLSTM_Attention.neg_ans_glove_seq_lengths:cc3})    \n",
    "      cum_loss+=result[1];\n",
    "      cum_pos_loss+=result[2];\n",
    "      cum_neg_loss+=result[3];\n",
    "      frm+=TRAINING_BATCH_SIZE;\n",
    "      progressBar(frm,n_train,['loss','pos_loss','neg_loss'],[result[1],result[2],result[3]]);\n",
    "    print('\\nTRAINING || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))\n",
    "    saver.save(sess, './'+model_save_folder+'/epoch_model_{}.ckpt'.format(epoch));\n",
    "    # VALIDATE\n",
    "    if epoch!=0 and epoch%VALIDATION_EVERY==0:\n",
    "      #saver = tf.train.Saver();\n",
    "      #saver.restore(val_sess, './'+model_save_folder+'/epoch_model_20.ckpt');\n",
    "      n_batches = int(np.floor(n_val/VALIDATION_BATCH_SIZE));\n",
    "      cum_loss = 0;\n",
    "      cum_pos_loss = 0;\n",
    "      cum_neg_loss = 0;\n",
    "      frm=0;\n",
    "      for i in range(n_batches):\n",
    "        batch_query_ids = validation_query_ids[frm:frm+VALIDATION_BATCH_SIZE];\n",
    "        batch_queries = [];\n",
    "        batch_pos = [];\n",
    "        batch_neg = [];\n",
    "        batch_arg_neg = [];\n",
    "        # VALIDATE: Find the most challenging negative!\n",
    "        if sampling_choice=='MAX': #get j's which are maximums in their negative sets\n",
    "            unfort_index = 0;\n",
    "            while(unfort_index<len(batch_query_ids)):\n",
    "                small_batch_query_ids = batch_query_ids[unfort_index:np.min([unfort_index+3,len(batch_query_ids)])];\n",
    "                dummy_batch_queries = [];\n",
    "                dummy_batch_neg = [];\n",
    "                dummy_batch_neg_n = [];\n",
    "                for query_id in small_batch_query_ids:\n",
    "                  dummy_batch_queries.append(data_dict[query_id]['query']);\n",
    "                  for jj in data_dict[query_id]['negs']:\n",
    "                    dummy_batch_neg.append(id2passage[jj]);\n",
    "                  dummy_batch_neg_n.append(len(data_dict[query_id]['negs']))        \n",
    "                #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "                aa1, _, aa3, aa4, aa5 = make_glove_and_elmo_batch_data(dummy_batch_queries,infer_max_len=True)\n",
    "                bb1, _, bb3, bb4, bb5 = make_glove_and_elmo_batch_data(dummy_batch_neg,infer_max_len=True)\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = [], [], [], [];\n",
    "                for ii,n_times in enumerate(dummy_batch_neg_n): # to tile x and y for thses many times \n",
    "                  aa1_new.append(np.tile(aa1[ii],(n_times,1)));\n",
    "                  aa3_new.append(np.tile(aa3[ii],(n_times,1)));\n",
    "                  aa4_new.append(np.tile(aa4[ii],(n_times,1)));\n",
    "                  aa5_new.append(np.tile(aa5[ii],(n_times,1)));\n",
    "                aa1_new, aa3_new, aa4_new, aa5_new = np.vstack(aa1_new), np.vstack(aa3_new), np.vstack(aa4_new), np.vstack(aa5_new);\n",
    "                aa5_new = np.squeeze(aa5_new) # making it 1-dim\n",
    "                result = sess.run([biLSTM_Attention.n_score],feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                                                    biLSTM_Attention.query_glove:aa1_new,\n",
    "                                                                    biLSTM_Attention.query_elmo:aa3_new,\n",
    "                                                                    biLSTM_Attention.query_bmask:aa4_new,\n",
    "                                                                    biLSTM_Attention.query_elmo_tokens:aa5_new,\n",
    "                                                                    biLSTM_Attention.neg_ans_glove:bb1,\n",
    "                                                                    biLSTM_Attention.neg_ans_elmo:bb3,\n",
    "                                                                    biLSTM_Attention.neg_ans_bmask:bb4,\n",
    "                                                                    biLSTM_Attention.neg_ans_elmo_tokens:bb5})\n",
    "                result = result[0]; # array of batch size i.e. (BS,1)\n",
    "                k=0;\n",
    "                for n_times in dummy_batch_neg_n: # because no of neg answers per query is variable\n",
    "                  batch_arg_neg.append(np.argmax(result[k:k+n_times]));\n",
    "                  k+=n_times;\n",
    "                unfort_index+=3;\n",
    "        # VALIDATE: compute loss only\n",
    "        if sampling_choice=='MAX':\n",
    "          assert(len(batch_arg_neg)==len(batch_query_ids))\n",
    "        for i_id, query_id in enumerate(batch_query_ids):\n",
    "          batch_queries.append(data_dict[query_id]['query']);\n",
    "          batch_pos.append(id2passage[data_dict[query_id]['pos']]);\n",
    "          this_negative = id2passage[data_dict[query_id]['negs'][batch_arg_neg[i_id]]] if sampling_choice=='MAX' else id2passage[np.random.choice(data_dict[query_id]['negs'])];\n",
    "          batch_neg.append(this_negative);\n",
    "        #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "        #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "        aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "        bb1, _, bb3 = make_glove_batch_data(batch_pos,infer_max_len=True) \n",
    "        cc1, _, cc3 = make_glove_batch_data(batch_neg,infer_max_len=True)\n",
    "        result = sess.run([biLSTM_Attention.print_losses, biLSTM_Attention.print_p_score_loss, biLSTM_Attention.print_n_score_loss],\n",
    "                          feed_dict={biLSTM_Attention.keep_prob: 0.8,\n",
    "                                     biLSTM_Attention.reg_lambda: 0.00001,\n",
    "                                     biLSTM_Attention.query_glove:aa1,\n",
    "                                     biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                     biLSTM_Attention.neg_ans_glove:cc1,\n",
    "                                     biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                     biLSTM_Attention.pos_ans_glove_seq_lengths:bb3,\n",
    "                                     biLSTM_Attention.neg_ans_glove_seq_lengths:cc3}) \n",
    "        cum_loss+=result[0];\n",
    "        cum_pos_loss+=result[1];\n",
    "        cum_neg_loss+=result[2];\n",
    "        frm+=VALIDATION_BATCH_SIZE;\n",
    "        progressBar(frm,n_val,['loss','pos_loss','neg_loss'],[result[0],result[1],result[2]]);\n",
    "      print('\\nVALIDATION || mean loss: {} | mean pos loss: {} | mean neg loss: {}'.format(cum_loss/n_batches,cum_pos_loss/n_batches,cum_neg_loss/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvBjcYVxr9QS"
   },
   "outputs": [],
   "source": [
    "# args\n",
    "EVAL_BATCH_SIZE = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hWAGkaMR66_1",
    "outputId": "7ddcb9bd-d06a-4e15-d149-dd416bac82da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df_test = pd.read_csv(\"eval1_unlabelled.tsv\", sep= '\\t', header=None)\n",
    "print('Eval Data Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4m3eUkR57Ccy",
    "outputId": "9a01a17f-30d4-407e-991e-2cd2696c9fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./biLSTM_Attention/epoch_model_10.ckpt\n",
      "Percent: [------------------->][104169/104170] 100% ||  : -1.000000 | "
     ]
    }
   ],
   "source": [
    "# get scores\n",
    "outputs = [];\n",
    "with tf.Session(config=tf_sess_config) as sess:\n",
    "  saver = tf.train.Saver();\n",
    "  saver.restore(sess, r'./'+model_save_folder+'/epoch_model_10.ckpt');\n",
    "  #saver.restore(sess, r'./'+model_save_folder+'/batch_model.ckpt');\n",
    "  for i, row in df_test.iterrows():\n",
    "    if i==0:\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    if i!=0 and i%EVAL_BATCH_SIZE==0:    \n",
    "      #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "      #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "      aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "      bb1, _, bb3 = make_glove_batch_data(batch_passages,infer_max_len=True)\n",
    "      result = sess.run([biLSTM_Attention.p_score],\n",
    "                        feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                   biLSTM_Attention.query_glove:aa1,\n",
    "                                   biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                   biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                   biLSTM_Attention.pos_ans_glove_seq_lengths:bb3})\n",
    "      outputs.append(result[0].tolist());\n",
    "      progressBar(i,df_test.shape[0]);\n",
    "      batch_queries = [];\n",
    "      batch_passages = [];\n",
    "    query, passage = row[1], row[2];\n",
    "    batch_queries.append(query);\n",
    "    batch_passages.append(passage);\n",
    "  if batch_queries:\n",
    "    #glove_arr, glove_bmasks, elmo_arr, elmo_bmasks, n_tokens = make_glove_and_elmo_batch_data(test_batch)\n",
    "    #glove_arr, glove_bmasks, n_tokens = make_glove_batch_data(test_batch) \n",
    "    aa1, _, aa3 = make_glove_batch_data(batch_queries,infer_max_len=True)\n",
    "    bb1, _, bb3 = make_glove_batch_data(batch_passages,infer_max_len=True)\n",
    "    result = sess.run([biLSTM_Attention.p_score],\n",
    "                      feed_dict={biLSTM_Attention.keep_prob: 1.0,\n",
    "                                 biLSTM_Attention.query_glove:aa1,\n",
    "                                 biLSTM_Attention.pos_ans_glove:bb1,\n",
    "                                 biLSTM_Attention.query_glove_seq_lengths:aa3,\n",
    "                                 biLSTM_Attention.pos_ans_glove_seq_lengths:bb3})\n",
    "    outputs.append(result[0].tolist());\n",
    "    progressBar(i,df_test.shape[0]);\n",
    "    batch_queries = [];\n",
    "    batch_passages = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ = np.hstack(outputs)\n",
    "df_test['pred'] = outputs_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lol = np.hstack(outputs)\n",
    "#len(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlS56pE97ILG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104170it [00:06, 15880.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# re-using code from Dileep\n",
    "outfilename = 'biLSTM_Attention_epoch_10.tsv'\n",
    "with open(outfilename,\"w\",encoding=\"utf-8\") as fw:\n",
    "  import math\n",
    "  linelist = []\n",
    "  tempscores = []\n",
    "  for idx, row in tqdm(df_test.iterrows()):\n",
    "      tempscores.append(row['pred'])\n",
    "      #tempscores.append(str(row['pred']))\n",
    "      if((idx +1)%10==0):\n",
    "          tempscores-=np.min(tempscores);\n",
    "          tempscores = [math.exp(s) for s in tempscores];\n",
    "          expsum = sum(tempscores)\n",
    "          tempscores = [str(s/expsum) for s in tempscores]\n",
    "          scoreString = \"\\t\".join(tempscores)\n",
    "          qid = str(row[0])\n",
    "          fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "          tempscores=[]\n",
    "      #if(idx%10000==0):\n",
    "      #    print(idx)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "0iKkfYSdPHM0",
    "evEnVCATnvOG",
    "8T70ZlM4LTnx",
    "67CAw6pZ_Rnm"
   ],
   "name": "MSAIC_2018.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
