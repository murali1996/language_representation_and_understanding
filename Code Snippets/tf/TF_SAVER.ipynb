{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Save using tf saver\n",
    "# Only one default 'checkpoint' file is created per folder\n",
    "# .meta file contains the information about the graph, i.e. how different vars are interacting\n",
    "# .ckpt.data contains mapping of vars to their tensor values\n",
    "# .ckpt.index gives indexing for lookup\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference btw name_scope and variable_scope\n",
    "# https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n",
    "# When the scope is specified with tf.name_scope(), the scope name is not added as header to the underlying vars and moreover,...\n",
    "  # ...such scope name cannot be used for collecting specific graph keys like in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name_scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving a graph vars and re-loading those vars under a new graph name\n",
    "class some_class(object):\n",
    "    def __init__(self):\n",
    "        self.w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "some_graph = tf.Graph();\n",
    "with some_graph.as_default():\n",
    "    some_object = some_class();\n",
    "with tf.Session(graph=some_graph) as sess:\n",
    "    saver = tf.train.Saver();\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    print(sess.run([some_object.w1])[0]);\n",
    "    saver.save(sess, './w1', global_step=100)\n",
    "\n",
    "some_new_graph = tf.Graph();\n",
    "with some_new_graph.as_default():\n",
    "    some_new_object = some_class();\n",
    "with tf.Session(graph=some_new_graph) as sess:\n",
    "    new_saver = tf.train.Saver();\n",
    "    new_saver.restore(sess,'./w1-100');\n",
    "    w1_new = some_new_graph.get_tensor_by_name('w1:0');\n",
    "    print(sess.run([w1_new])[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# While reloading vars, the tf names matter and not the names you used to refer them in python script!\n",
    "some_graph = tf.Graph();\n",
    "with some_graph.as_default():\n",
    "    w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "with tf.Session(graph=some_graph) as sess:\n",
    "    sess.run(w1.initializer)\n",
    "    saver = tf.train.Saver();\n",
    "    saver.save(sess, './w1', global_step=1200)\n",
    "    print(sess.run([w1])[0])\n",
    "\n",
    "some_new_graph = tf.Graph();\n",
    "with some_new_graph.as_default():\n",
    "    w2 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "with tf.Session(graph=some_new_graph) as sess:\n",
    "    new_saver = tf.train.Saver();\n",
    "    new_saver.restore(sess, './w1-1200')\n",
    "    print(sess.run([w2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# While reloading a specific var fraom a checkpoint, its shape must match. Else you'll get InvalidArgumentError\n",
    "# InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [2,5] rhs shape= [1,5]\n",
    "\n",
    "# Creating a graph and saving vars\n",
    "class some_class(object):\n",
    "    def __init__(self):\n",
    "        self.w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "some_graph = tf.Graph();\n",
    "with some_graph.as_default():\n",
    "    some_object = some_class();\n",
    "with tf.Session(graph=some_graph) as sess:\n",
    "    saver = tf.train.Saver();\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    print(sess.run([some_object.w1])[0]);\n",
    "    saver.save(sess, './w1', global_step=500)\n",
    "# Creating another graph with same name but with different size\n",
    "class some_class(object):\n",
    "    def __init__(self):\n",
    "        self.w1 = tf.get_variable(\"w1\", shape=[2,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "some_new_graph = tf.Graph();\n",
    "with some_new_graph.as_default():\n",
    "    some_new_object = some_class();\n",
    "with tf.Session(graph=some_new_graph) as sess:\n",
    "    new_saver = tf.train.Saver();\n",
    "    new_saver.restore(sess,'./w1-500');\n",
    "    w1_new = some_new_graph.get_tensor_by_name('w1:0');\n",
    "    print(sess.run([w1_new])[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# While restoring, your new graph must have vars which are a subset of checkpoint vars_list. Else NotFoundError pops up\n",
    "# NotFoundError: Key w2 not found in checkpoint\n",
    "\n",
    "tf.reset_default_graph();\n",
    "# Creating a graph and saving vars\n",
    "class some_class(object):\n",
    "    def __init__(self):\n",
    "        self.w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "        self.w10 = tf.get_variable(\"w10\", shape=[7,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "        self.w100 = tf.get_variable(\"w100\", shape=[14,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "some_graph = tf.Graph();\n",
    "with some_graph.as_default():\n",
    "    some_object = some_class();\n",
    "with tf.Session(graph=some_graph) as sess:\n",
    "    saver = tf.train.Saver();\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    print(sess.run([some_object.w1])[0]);\n",
    "    saver.save(sess, './first_graph', global_step=100)\n",
    "# Creating another graph with some vars same as previous graph\n",
    "class some_other_class(object):\n",
    "    def __init__(self):\n",
    "        # Run this...\n",
    "        self.w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "        self.w1_0 = tf.get_variable(\"w10\", shape=[7,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "        self.w2 = tf.get_variable(\"w2\", shape=[3,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "some_new_graph = tf.Graph();\n",
    "with some_new_graph.as_default():\n",
    "    some_new_object = some_other_class();\n",
    "with tf.Session(graph=some_new_graph) as sess:\n",
    "    new_saver = tf.train.Saver();\n",
    "    new_saver.restore(sess,'./first_graph-100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subset vars loading; loading vars under a specific name scope in tf graph; restroring specific variables based on name matching\n",
    "# https://stackoverflow.com/questions/41621071/restore-subset-of-variables-in-tensorflow and https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__\n",
    "# We'll make a collection of the requisite vars under a given name_scope and save them seperately\n",
    "\n",
    "tf.reset_default_graph();\n",
    "# Creating a graph and saving vars\n",
    "class some_class(object):\n",
    "    def __init__(self):\n",
    "        self.base_scope = 'base_model_0';\n",
    "        self.classifier_scope = 'classifier_model_0';\n",
    "        with tf.variable_scope(self.base_scope, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope('layer_1'):\n",
    "                w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            with tf.variable_scope('layer_2'):\n",
    "                w2 = tf.get_variable(\"w2\", shape=[5,100], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w3 = tf.matmul(w1, w2, name='w3')\n",
    "        with tf.variable_scope(self.classifier_scope, reuse=tf.AUTO_REUSE):\n",
    "            w4 = tf.get_variable(\"w4\", shape=[100,200], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w5 = tf.matmul(self.w3, w4, name='w5')\n",
    "some_graph = tf.Graph();\n",
    "with some_graph.as_default():\n",
    "    some_object = some_class();\n",
    "with tf.Session(graph=some_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer());\n",
    "    w3_out,w5_out = sess.run([some_object.w3,some_object.w5]);\n",
    "    print(w3_out, w5_out);\n",
    "    global_step = 550; # global_step = self.global_step; # Ideally, you update global steps in you optimizer code\n",
    "    var_list_base = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=some_object.base_scope)\n",
    "    print('Printing base list...')\n",
    "    for var in var_list_base:\n",
    "        print(var.name)\n",
    "    saver_base = tf.train.Saver(var_list=var_list_base);\n",
    "    saver_base.save(sess, './base_graph', global_step=global_step)\n",
    "    var_list_full = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    saver_full = tf.train.Saver(var_list=var_list_full);\n",
    "    saver_full.save(sess, './full_graph', global_step=global_step)\n",
    "    print('Printing full list...')\n",
    "    for var in var_list_full:\n",
    "        print(var.name)\n",
    "# Creating another graph with some vars same as previous graph but with DIFFERENT scope name\n",
    "# returns NotFoundError: Key base_model_1/w1 not found in checkpoint\n",
    "class some_other_class(object):\n",
    "    def __init__(self):\n",
    "        self.base_scope = 'base_model_1';\n",
    "        self.classifier_scope = 'classifier_model_1';\n",
    "        with tf.variable_scope(self.base_scope, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope('layer_1'):\n",
    "                w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            with tf.variable_scope('layer_2'):\n",
    "                w2 = tf.get_variable(\"w2\", shape=[5,100], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w3 = tf.matmul(w1, w2, name='w3')\n",
    "        with tf.variable_scope(self.classifier_scope, reuse=tf.AUTO_REUSE):\n",
    "            w4 = tf.get_variable(\"w4\", shape=[100,33], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w5 = tf.matmul(self.w3, w4, name='w5')\n",
    "some_new_graph = tf.Graph();\n",
    "with some_new_graph.as_default():\n",
    "    some_new_object = some_other_class();\n",
    "with tf.Session(graph=some_new_graph) as sess:\n",
    "    var_list_base = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=some_new_object.base_scope)\n",
    "    new_saver_base = tf.train.Saver(var_list=var_list_base);\n",
    "    new_saver_base.restore(sess,'./base_graph-550');\n",
    "# Creating another graph with some vars same as previous graph but with SAME scope name\n",
    "# returns no error unlike previous snippet!\n",
    "class some_other_other_class(object):\n",
    "    def __init__(self):\n",
    "        self.base_scope = 'base_model_0';\n",
    "        self.classifier_scope = 'classifier_model_2';\n",
    "        with tf.variable_scope(self.base_scope, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope('layer_1'):\n",
    "                w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            with tf.variable_scope('layer_2'):\n",
    "                w2 = tf.get_variable(\"w2\", shape=[5,100], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w3 = tf.matmul(w1, w2, name='w3')\n",
    "        with tf.variable_scope(self.classifier_scope, reuse=tf.AUTO_REUSE):\n",
    "            w4 = tf.get_variable(\"w4\", shape=[100,200], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w5 = tf.matmul(self.w3, w4, name='w5')\n",
    "some_new_new_graph = tf.Graph();\n",
    "with some_new_new_graph.as_default():\n",
    "    some_new_new_object = some_other_other_class();\n",
    "with tf.Session(graph=some_new_new_graph) as sess:\n",
    "    var_list_base = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=some_new_new_object.base_scope)\n",
    "    new_saver_base = tf.train.Saver(var_list=var_list_base);\n",
    "    new_saver_base.restore(sess,'./full_graph-550'); # <--- Watch the name here and compare with next snippet\n",
    "# Creating another graph with some vars same as previous graph but with SAME scope name\n",
    "# returns no error unlike previous snippet!\n",
    "class some_other_other_class(object):\n",
    "    def __init__(self):\n",
    "        self.base_scope = 'base_model_0';\n",
    "        self.classifier_scope = 'classifier_model_2';\n",
    "        with tf.variable_scope(self.base_scope, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope('layer_1'):\n",
    "                w1 = tf.get_variable(\"w1\", shape=[1,5], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            with tf.variable_scope('layer_2'):\n",
    "                w2 = tf.get_variable(\"w2\", shape=[5,100], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w3 = tf.matmul(w1, w2, name='w3')\n",
    "        with tf.variable_scope(self.classifier_scope, reuse=tf.AUTO_REUSE):\n",
    "            w4 = tf.get_variable(\"w4\", shape=[100,200], initializer=tf.random_uniform_initializer(minval=-5, maxval=5))\n",
    "            self.w5 = tf.matmul(self.w3, w4, name='w5')\n",
    "some_new_new_graph = tf.Graph();\n",
    "with some_new_new_graph.as_default():\n",
    "    some_new_new_object = some_other_other_class();\n",
    "with tf.Session(graph=some_new_new_graph) as sess:\n",
    "    var_list_base = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=some_new_new_object.base_scope)\n",
    "    new_saver_base = tf.train.Saver(var_list=var_list_base);\n",
    "    new_saver_base.restore(sess,'./full_graph-550');  # <--- Changes made here\n",
    "    w3_out = sess.run([some_new_new_object.w3]);\n",
    "    print(w3_out);\n",
    "# This means, while restoring, just mark the appropriate variable_scope that has to be restored by your saver.\n",
    "# While saving, you can save, as usual, the entire graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
