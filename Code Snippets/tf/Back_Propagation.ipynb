{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Optimization in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For gradient descent to minimize loss in tensorflow, you select\n",
    "- the variable list to be updated\n",
    "- the loss function and the optimizer to carry out the update step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.<optimizer>.minimize(loss, var_list)\n",
    "- compute_gradients: This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "- apply_gradients: This is the second part of minimize(). It returns an Operation that applies gradients\n",
    "- optimizer.compute_gradients wraps tf.gradients(), as you can see here. It does additional asserts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Simple Update of all Variables\n",
    "vars = tf.trainable_variables();\n",
    "gradients = tf.gradients(loss, trainable_params);\n",
    "update_step = tf.train.RMSPropOptimizer(configure.learning_rate).apply_gradients(zip(gradients, trainable_params), global_step=global_step_val);\n",
    "\n",
    "# 2. Update with Clipped gradients\n",
    "clipped_gradients, global_norm_value = tf.clip_by_global_norm(gradients, configure.max_gradient_norm); \n",
    "update_step = tf.train.RMSPropOptimizer(configure.learning_rate).apply_gradients(zip(clipped_gradients, trainable_params), global_step=global_step_val);\n",
    "\n",
    "# 3. Update Selected Variables\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator\")\n",
    "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(gen_loss, gen_vars) # G Train step\n",
    "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(disc_loss, disc_vars) # D Train step\n",
    "\n",
    "# 4. Other way of clipping gradients\n",
    "def _ClipIfNotNone(grad):\n",
    "    if grad is None:\n",
    "      return grad\n",
    "    #grad = tf.clip_by_value(grad, -10, 10, name=None)\n",
    "    grad = tf.clip_by_norm(grad, 5.0)\n",
    "    return grad\n",
    "clipped_gradients = [(_ClipIfNotNone(grad), var) for grad, var in gradients]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
