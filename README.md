# Interesting Papers in NLP

![alt text][how_i_met_your_paper]

---
---
---
## Selected Word Embedding Techniques
1. [*Word2Vec*, Efficient Estimation of Word Representations in Vector Space, Mikolov et al. 2013a][Mikolov et al. 2013a]
1. [*Word2Vec*, Distributed Representations of Words and Phrases and their Compositionality, Mikolov et al. 2013b][Mikolov et al. 2013b]
1. [*GloVe*, GloVe: Global Vectors for Word Representation, Pennington et al. 2014][Pennington et al. 2014]
1. [*FastText*, Enriching Word Vectors with Subword Information, Bojanowski et al. 2016][Bojanowski et al. 2016]
1. [*ELMo*, Deep contextualized word representations, Peters et al. 2018][Peters et al. 2018] 
1. [*FLAIR*, Contextual String Embeddings for Sequence Labeling, Akbik et al. 2018][Akbik et al. 2018] [[CODE]](https://github.com/zalandoresearch/flair)
1. [Learning Gender-Neutral Word Embeddings, Zhao et al. 2018][Zhao et al. 2018]
## Selected Sentence Encoding Techniques
1. [Skip-Thought Vectors, Kiros et al. 2015][Kiros et al. 2015]
1. [A Structured Self-attentive Sentence Embedding, Lin et al. 2017][Lin et al. 2017]
1. [*InferSent*, Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, Conneau eta al. 2017][Conneau eta al. 2017]
1. [*USE*, Universal Sentence Encoder, Cer et al. 2018][Cer et al. 2018]
1. [*GenSen*, Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning, Subramanian et al. 2018][Subramanian et al. 2018]
1. [No Training Required: Exploring Random Encoders for Sentence Classification, Wieting et al. 2019][Wieting et al. 2019]
## Objectives Built On Top of Word/Character/BPE vectors or Built From Scratch
1. [Investigating Capsule Networks with Dynamic Routing for Text Classification, Zhao et al. 2018][Zhao et al. 2018]
1. [Zero-shot User Intent Detection via Capsule Neural Networks, Xia et al. 2018][Xia et al. 2018]
---
---
---
## NLU Benchmarks
1. [SentEval: An Evaluation Toolkit for Universal Sentence Representations, Conneau & Kiela 2018][Conneau & Kiela 2018] [[Site]](https://github.com/facebookresearch/SentEval)
1. [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, Wang et al. 2018][Wang et al. 2018] [[Site]](https://gluebenchmark.com/leaderboard)
## Language Modeling for NLU
1. [Semi-supervised Sequence Learning, Andrew and Quoc 2015][Andrew and Quoc 2015]
1. [*ULM-FiT*, Universal Language Model Fine-tuning for Text Classification, Howard and Ruder 2018][Howard and Ruder 2018]
1. [*ELMo*, Deep contextualized word representations, Peters et al. 2018][Peters et al. 2018] 
1. [*GPT-1 aka OpenAI Transformer*, Improving Language Understanding by Generative Pre-Training, Radford et al. 2018][Radford et al. 2018]
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al. 2018][Devlin et al. 2018] [[SLIDES]](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)
1. [Looking for ELMo's Friends: Sentence-Level Pretraining Beyond Language Modeling, Bowman et al. 2018][Bowman et al. 2018]
1. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang et al. 2019][Zihang et al. 2019]
1. [*DistillBERT*, Distilling Task-Specific Knowledge from BERT into Simple Neural Networks, Tang et al. 2019][Tang et al. 2019]
## Multi-Task Learning for NLU
1. [*GenSen*, Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning, Subramanian et al. 2018][Subramanian et al. 2018]
1. [*decaNLP*, The Natural Language Decathlon: Multitask Learning as Question Answering, McCann et al. 2018][McCann et al. 2018]
1. [*HMTL*, A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks, Victor et al. 2018][Victor et al. 2018]
1. [*GPT-2*, Language Models are Unsupervised Multitask Learners, Radford et al. 2019][Radford et al. 2019]
## Objectives Built On Top Of Pre-trained Models
1. [Practical Text Classification With Large Pre-Trained Language Models, NeelKant et al. 2018][NeelKant et al. 2018]
1. [BERT for Joint Intent Classification and Slot Filling, Chen et al. 2019][Chen et al. 2019]
1. [SciBERT: Pretrained Contextualized Embeddings for Scientific Text, Beltagy et al. 2019][Beltagy et al. 2019]
---
---
---
## Selected Cross-Lingual Tasks' Works like NMT, XNLI, etc.
1. [*Transformer*, Attention Is All You Need, Vaswami et al. 2017][Vaswami et al. 2017]
1. [Neural Machine Translation of Rare Words with Subword Units, Sennrich et al. 2015][Sennrich et al. 2015]
1. [Achieving Human Parity on Automatic Chinese to English News Translation, Hassan et al. 2018][Hassan et al. 2018]
1. [XNLI: Evaluating Cross-lingual Sentence Representations, Conneau eta al. 2018c][Conneau eta al. 2018c]
1. [Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond, Artetxe et al. 2018][Artetxe et al. 2018]
1. [Cross-lingual Language Model Pretraining, Lample et al. 2019][Lample et al. 2019]
---
---
---
## Miscellaneous
1. [*Capsule-Nets*, Dynamic Routing Between Capsules, Sabour et al. 2017][Sabour et al. 2017]
---
---
---

# Downloads
1. [*Word2Vec*](https://github.com/mmihaltz/word2vec-GoogleNews-vectors/)
1. [*Glove*](https://nlp.stanford.edu/projects/glove/)

# Some cool articles/blogs/sites for nlp
- [http://nlpprogress.com/](http://nlpprogress.com/),
  [NLP | ML Explained](http://mlexplained.com/category/nlp/),
  [Ruder's Blogs](http://ruder.io) </br>
- [Awesome-NLP](https://github.com/keon/awesome-nlp),
  [NLP-Tutorial](https://github.com/graykode/nlp-tutorial),
  [lazynlp](https://github.com/chiphuyen/lazynlp) </br>
- [Illustrated BERT](http://jalammar.github.io/illustrated-bert/),
  [OpenAi GPT-1](https://openai.com/blog/language-unsupervised/),
  [OpenAi GPT-2](https://openai.com/blog/better-language-models/),
  [The Natural Language Decathlon](https://blog.einstein.ai/the-natural-language-decathlon/),
  [ULMFit](https://yashuseth.blog/2018/06/17/understanding-universal-language-model-fine-tuning-ulmfit/) </br>
- [Regularization Techniques for NLP](http://mlexplained.com/2018/03/02/regularization-techniques-for-natural-language-processing-with-code-examples/),
  [Important AI papers 2018 TOPBOTS](https://www.topbots.com/most-important-ai-research-papers-2018/),
  [Chat Smarter with Allo](https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html) </br>

# Food For Thought
1. How good do ranking algorithms, the ones with pointwise/pairwise/listwise learning paradigms, perform when the no. of test classes at the infernece time grow massively? KG Reasoning using Translational/Bilinear/DL techniques is one important area under consideration.
1. While the chosen neural achitecture is important, the techniques used for training the problem objective e.g.[*Word2Vec*][Mikolov et al. 2013b] or the techniques used while doing loss optimization e.g.[*OpenAI Transformer*][Radford et al. 2018] play a significant role in both fast as well as a good convergence.
1. Commonality between Language Modelling, Machine Translation and Word2vec: All of them have a huge vocabulary size at the output and there is a need to alleviate computing of the huge sized softmax layer! See [Ruder's page](http://ruder.io/word-embeddings-softmax/index.html) for a quick-read.

# Quick Bites
1. Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of symbols (originally bytes) in a given dataset with a single unused symbol. In each iteration, the algorithm finds the most frequent (adjacent) pair of symbols, each can be constructed of a single character or a sequence of characters, and merged them to create a new symbol. All occurences of the selected pair are then replaced with the new symbol before the next iteration. Eventually, frequent sequence of characters, up to a whole word, are replaced with a single symbol, until the algorithm reaches the defined number of iterations (50k in this paper). During inference, if a word isn’t part of the BPE’s pre-built dictionary, it will be split into subwords that are.
An example code of BPE can be found here. https://gist.github.com/ranihorev/6ba9a88c9e7401b603cd483dd767e783
1. Models can be trained on SNLI in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).

[how_i_met_your_paper]: https://github.com/murali1996/nlp/blob/master/images/how_i_met_your_paper.png "Connections"

[Mikolov et al. 2013a]: https://arxiv.org/abs/1301.3781
[Mikolov et al. 2013b]: https://arxiv.org/abs/1310.4546
[Pennington et al. 2014]: https://www.aclweb.org/anthology/D14-1162
[Bojanowski et al. 2016]: https://arxiv.org/abs/1607.04606
[Peters et al. 2018]: https://arxiv.org/abs/1802.05365
[Akbik et al. 2018]: http://alanakbik.github.io/papers/coling2018.pdf
[Lin et al. 2017]: https://arxiv.org/abs/1703.03130
[Vaswami et al. 2017]: https://arxiv.org/pdf/1706.03762.pdf
[Radford et al. 2018]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
[Howard and Ruder 2018]: https://arxiv.org/abs/1801.06146
[Devlin et al. 2018]:https://arxiv.org/abs/1810.04805
[Radford et al. 2019]: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
[Sabour et al. 2017]: https://arxiv.org/abs/1710.09829
[Xia et al. 2018]: https://arxiv.org/abs/1809.00385
[Cer et al. 2018]: https://arxiv.org/pdf/1803.11175.pdf
[Subramanian et al. 2018]: https://arxiv.org/abs/1804.00079
[McCann et al. 2018]: https://arxiv.org/abs/1806.08730
[Zihang et al. 2019]: https://arxiv.org/abs/1901.02860v2
[Victor et al. 2018]: https://arxiv.org/abs/1811.06031
[Andrew and Quoc 2015]: https://arxiv.org/abs/1511.01432
[Chen et al. 2019]: https://arxiv.org/abs/1902.10909
[Zhao et al. 2018]: https://arxiv.org/abs/1804.00538
[Conneau eta al. 2017]: https://arxiv.org/abs/1705.02364
[Wieting et al. 2019]: https://arxiv.org/abs/1901.10444
[Beltagy et al. 2019]: https://arxiv.org/abs/1903.10676
[NeelKant et al. 2018]: https://arxiv.org/abs/1812.01207
[Wang et al. 2018]: https://arxiv.org/abs/1804.07461
[Hassan et al. 2018]: https://arxiv.org/abs/1803.05567
[Bowman et al. 2018]: https://arxiv.org/abs/1812.10860
[Tang et al. 2019]: https://arxiv.org/abs/1903.12136
[Sennrich et al. 2015]: https://arxiv.org/abs/1508.07909
[Artetxe et al. 2018]: https://arxiv.org/abs/1812.10464
[Zhao et al. 2018]: https://arxiv.org/abs/1809.01496
[Lample et al. 2019]: https://arxiv.org/abs/1901.07291
[Conneau eta al. 2018c]: https://arxiv.org/abs/1809.05053
[Kiros et al. 2015]: https://arxiv.org/abs/1506.06726
[Conneau & Kiela 2018]: https://arxiv.org/abs/1803.05449

[Illustrated BERT]: http://jalammar.github.io/illustrated-bert/
[OpenAi GPT-1]: https://openai.com/blog/language-unsupervised/
[OpenAi GPT-2]: https://openai.com/blog/better-language-models/
[The Natural Language Decathlon]: https://blog.einstein.ai/the-natural-language-decathlon/
[ULMFit]: https://yashuseth.blog/2018/06/17/understanding-universal-language-model-fine-tuning-ulmfit/
[Important AI papers 2018 TOPBOTS]: https://www.topbots.com/most-important-ai-research-papers-2018/
[Chat Smarter with Allo]: https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html
[Regularization Techniques for NLP]: http://mlexplained.com/2018/03/02/regularization-techniques-for-natural-language-processing-with-code-examples/
